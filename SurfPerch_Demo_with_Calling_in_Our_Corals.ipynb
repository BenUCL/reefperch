{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpdd27Y0ff6gGqjAf+G3bY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenUCL/surfperch/blob/surfperch/SurfPerch_Demo_with_Calling_in_Our_Corals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SurfPerch Tutorial with Calling in Our Corals** üê†üê¶\n",
        "\n",
        "This notebook was adapted from the original NeurIPS workshop tutorial 'CCAI Agile Modeling for Bioacoustic Monitoring Tutorial', authored with equal contribution by:\n",
        "*   Jenny Hamer, Google DeepMind\n",
        "*   Rob Laber, Google Cloud\n",
        "*   Tom Denton, Google Research\n",
        "\n",
        "The adaptation presented here was authored by:\n",
        "* Ben Williams, UCL (Google DeepMind Student Researcher)"
      ],
      "metadata": {
        "id": "c-iKkbImw_I_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "*   [Introduction](#introduction)\n",
        "  - [Background](#background)\n",
        "  - [Calling in Our Corals](#cioc)\n",
        "  - [SurfPerch](#SurfPerch)\n",
        "  - [Glossary](#glossary)\n",
        "  - [Agile Modeling for Bioacoustics](#agile_modeling)\n",
        "  - [Software Requirements](#software-requirements)\n",
        "  - [Adapting to New Use Cases](#adapting)\n",
        "*   [Set Up](#set-up)\n",
        "*   [Methodology](#methodology)\n",
        "  - [Embed the Search Dataset](#embed_data)\n",
        "  - [Audio Similarity Search](#similarity-search)\n",
        "  - [Active Learning](#active-learning)\n",
        "*   [Results](#results)"
      ],
      "metadata": {
        "id": "5Vk0HPvDDIvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=introduction></a>\n",
        "# Introduction\n",
        "\n",
        "> (8 min read)\n",
        "\n"
      ],
      "metadata": {
        "id": "81Y68NsAHe6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=background></a>\n",
        "## Background\n",
        "\n",
        "Passive acoustic monitoring (PAM) can be used to collect valuable information on the state of marine habitats. However, this data can easily range from dozens to thousands of hours in size, becoming too large for marine scientists to manually analyse. Machine learning offers a powerful solution to this challenge.\n",
        "\n",
        "The goals of this notebook are:\n",
        "1. Provide marine scientists with a workflow that enables them to rapidly anaylse PAM data using machine learning.\n",
        "2. Guide users through how to build a highly accurate machine learning detector known as a 'classifier' for any given sound of interest in their dataset, starting from a single example.\n",
        "3. Demonstrate how this can be run entirely from this notebook using data uploaded to Google Drive.\n",
        "\n",
        "We demonstrate this using data gathered for the Calling in Our Corals project.\n"
      ],
      "metadata": {
        "id": "vHZcexFduQ9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=cioc></a>\n",
        "## Calling in Our Corals\n",
        "\n",
        "Coral reef habitats have rich and diverse soundscapes. However, a key challenge bioacousticians working on these habitats must overcome is identifying which sounds are of interest. For any given dataset, the source of most of sounds are typically unknown and undocumented making it challenging for marine bioacousticians to know where to begin.\n",
        "\n",
        "[Calling in Our Corals](https://artsandculture.google.com/experiment/calling-in-our-corals/zgFx1tMqeIZyTw?hl=en) (CIOC) is a collaboration started in 2023 between Google Arts and Culture and marine biologists hoping to overcome this challenge. The project is centred around a platform used to crowd source annotations for marine acoustic data. Over 24'000 unique users logged on to the platform and listened to a combined 400 hours of audio recorded from marine habitats in 10 different countries, annotating any periods they identified as containing sounds of interest. By taking the audio clips with the highest number of annotations for each dataset a diverse set of biological sounds were revealed from each.\n",
        "\n",
        "Here, we use some of these sounds from three datasets as an entry point into our machine learning pipeline. The results shared at the end only represents a preliminary analysis using a subset of the data as scientists continue to work on the outputs. However, we find this reveals interesting patterns between the soundscape and state of the habitats."
      ],
      "metadata": {
        "id": "PK9mjDu3C6DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Learn more about coral reef sounds here\n",
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "# YouTube video ID\n",
        "video_id = \"POITH02VVrw\"\n",
        "\n",
        "# Display the YouTube video in the notebook\n",
        "YouTubeVideo(video_id)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-863qtX_Uj2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=SurfPerch></a>\n",
        "## SurfPerch\n",
        "\n",
        "The Perch effort is comprised of a team of Google researchers focused on making bioacoustics more efficient, more effective and easier to do. So far, the core component of the teams worked has focused on terrestrial bioacouastic challenges. For example, development of the [Perch model](https://doi.org/10.1038/s41598-023-49989-z) which offers a powerful 'pretrained' neural network for terrestrial acoustic challenges.\n",
        "\n",
        "More recently, the team branched into the marine environment with the introduction of SurfPerch, a sister model to Perch which is optimised for coral reef habitats. To develop SurfPerch the team worked with key collaborators to assemble ReefSet, a library of annotated reef sounds. However, ReefSet is still modest in size compared to other bioacoustic fields. The secret to success of SurfPerch actually came from mixing ReefSet with the very same bird datasets used to train Perch, alongside more general audio from the Freesound 50k dataset. More details can be found in the models supporting research article (TODO add link).\n",
        "\n",
        "SurfPerch should offer a powerful tool for studying the acoustics of coral reefs and other aquatic habitats where fish and similar sounds are the primary target. This tutorial will walk you through how to use this from start to end.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WvwR_OmcH-bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Learn more about the Perch teams previous work here\n",
        "# YouTube video ID\n",
        "video_id = \"-d9IRea_P3M\"\n",
        "\n",
        "# Display the YouTube video in the notebook\n",
        "YouTubeVideo(video_id)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WS3ToUziNfN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=glossary></a>\n",
        "## Glossary üìñ\n",
        "\n",
        "We provide a high-level, intuitive description of some of the core components and machine learning concepts utilized this tutorial."
      ],
      "metadata": {
        "id": "3nNEcKJPFJ-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agile Modeling\n",
        "\n",
        "Agile modeling in the context of supervised machine learning can be described as ‚Äúthe process of turning any subjective visual concept into a computer vision model through real-time user-in-the-loop interactions‚Äù ([Stretcu et al. 2023](https://arxiv.org/abs/2302.12948)). In our setting, we go beyond the typical image and computer vision settings to explore the audio domain, where we use this **rapdily build an accurate classifier from a single labeled audio sample.**\n",
        "\n"
      ],
      "metadata": {
        "id": "Vkp4uuLQ0Ed2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning\n",
        "\n",
        "Transfer learning is a technique that relies on the reuse of a pretrained model on a new task (different dataset from which it was trained). This technique assumes that the pre-training of the model produces a useful representation and knowledge that can be extended to the new task/setting. In the context of this tutorial we use SurfPerch, a model pretrained on a large acoustic dataset of coral reef, bird and general audio clops to produce embeddings of a new soundscape dataset (passive recordings).\n",
        "\n",
        "\n",
        "**Intuition:** in the context of bioacoustics, transfer learning is useful for building a general representation that can translate to new sounds or geographical regions, particularly when labeled data is scarce. Using a pretrained model with transfer learning lowers the machine learning overhead for field experts and practitioners who seek to understand and interpret unlabeled data, such as that from passive acoustic monitoring (PAM)."
      ],
      "metadata": {
        "id": "nkzdCBz20FWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Active Learning\n",
        "\n",
        "Active Learning is a general term for an iterative, supervised ML technique that efficiently makes use of labeled data to ‚Äúlearn the labels‚Äù or classify unlabeled data. This learning setting is particularly helpful in scenarios where unlabeled data is abundant and producing labels for that data is costly, requires expert knowledge, lots of manual work, etc."
      ],
      "metadata": {
        "id": "ChmM_DhT0FTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings\n",
        "\n",
        "At their core, neural networks function by learning new representations of data that help make the underlying patterns in the dataset more obvious.  These representations are called **embeddings**.  Concretely, an embedding is a point (or vector) in a high dimensional space.  Data points with embeddings that are close to each other are likely to share salient traits."
      ],
      "metadata": {
        "id": "InEMACc60FRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=agile_modeling></a>\n",
        "## Agile Modeling for Bioacoustics\n",
        "\n",
        "The core problem this tutorial addresses lies in biodiversity monitoring using large-scale, unlabeled audio data, as collected by inexpensive passive acoustic monitoring (PAM) devices. We present an **integrated workflow** for analyzing large unlabeled bioacoustic datasets, adapting new [agile modeling techniques](https://arxiv.org/abs/2302.12948) to the audio domain. The tools we provide enable users to adapt a classifier for a novel class (species, specific call type, etc...) with minimal overhead.\n",
        "\n",
        "Our goal is to **allow experts to create classifiers for new sounds in less than an hour.**\n",
        "\n",
        "The overall flow has three main parts:\n",
        "* First, we use a high-quality *embedding model* to produce embeddings of a large unlabeled dataset, such as a conservation organization might collect. This produces a large table of embeddings, which can be easily joined back to the original audio. (For large datasets, this step may take a substantial amount of time, but only needs to be done once: All later steps make use of these precomputed embeddings.)\n",
        "\n",
        "* Next, we use audio similarity search to sift through the unlabeled dataset embeddings for sounds of interest. The user provides a query‚Äîan example of a sound category they are searching for, like a juvenile call for a particular species‚Äîwhich is also embedded, and we present the user with the most relevant dataset elements according to their embeddings' similarity to the query embedding. The user marks the retrieved dataset elements as relevant or irrelevant, which provides training data for the next step.\n",
        "   \n",
        "  - The similarity search over embeddings allows us to find similar audio examples, even when there is some variation, vocalizations are quiet, or there are overlapping sounds.\n",
        "  \n",
        "  - In the example below, we are able to find relevant examples of hairy woodpecker drumming sounds (blue) even in the presence of competing sounds (brown).  The [spectrogram](https://en.wikipedia.org/wiki/Spectrogram) on the left is a known vocalization of a hairy woodpecker.  The similarity search algorithm allows us to detect instances of this vocalizations in an audio context that may have\n",
        "  other background noise (the three spectrograms on the right, with background noise indicated in brown):\n",
        "  > ![search](https://docs.google.com/drawings/d/e/2PACX-1vSIwCEkHNEjtyh3PLhTa-4hTTaBoMnWGVXobr91rKeD7m52a4ES9K0JZ_tqltasUWctgLHnf2PrvtKU/pub?w=371&h=253)\n",
        "  \n",
        "\n",
        "\n",
        "* The final step is to use the training data obtained in the previous step as part of an active learning loop to train a classifier for the user's search query. During each iteration of the loop, we fit a simple classifier onto the training data. We use the trained classifier along with margin sampling to efficiently surface 'hard' examples that are then rated by the user as relevant or irrelevant to their query. Finally, the newly-rated data is added to the training data for the next iteration of the active learning loop. The fact that embeddings are only computed once on the unlabeled dataset means that each iteration of the loop is very fast to carry out.\n",
        "\n",
        "The core of this process is a high-quality model for bioacoustic data. [We have shown](https://arxiv.org/pdf/2307.06292.pdf) that the embeddings from our [global bird classification model](https://tfhub.dev/google/bird-vocalization-classifier/4) transfer efficiently to novel bioacoustic problems - including bird call-types, bats, and marine mammals - outperforming general audio foundation models. Now with SurfPerch, we present a new model that outperforms all others in the coral reef domain, and likely similar aquatic habitats. Additionally, because we work with precomputed embeddings, the classifiers are very quick to train and to apply to the unlabeled dataset."
      ],
      "metadata": {
        "id": "o-minijQ0FO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=software-requirements></a>\n",
        "## Software Requirements\n",
        "\n",
        "The majority of the code is contained in a publicly accessible repository hosted on github: https://github.com/google-research/perch.\n",
        "\n",
        "**Note:** This repository was formerly named 'chirp', so any references to 'chirp' in the code below are references to code in this Perch repository.\n",
        "\n",
        "The Perch codebase contains a variety of processing and machine learning tooling for working with audio datasets, with a focus on avian acoustics which we have also adapted for reef acoustics.  This tutorial demonstrates only some of the functionality provided by the Perch codebase.  **Therefore, there are section of this tutorial where we ask the reader to configure default values that are targeted to this specific use case and tutorial.  In order to keep this tutorial focused, we will selectively discuss only the most relevant parameters, and we refer the reader to the github repository for more background and context.**\n",
        "\n",
        "To use this tutorial, we rely on the off-the-shelf Google Colab runtime (Python 3 Google Compute Engine backend). This has the majority of the required packages already installed and meet the version requirements. If you are using this, then **no action is required**. Packages include:\n",
        "* python, version 3.10+\n",
        "* NumPy, version 1.2+\n",
        "* Tensorflow, version 2+\n",
        "* [tqdm](https://tqdm.github.io/), version 4.6+\n",
        "\n",
        "You should be able to use any of the three free provided backends available as of Fall 2023 (CPU, T4 GPU, or TPU)."
      ],
      "metadata": {
        "id": "sA2J55d00FMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=adapting></a>\n",
        "## Adapting for new use cases\n",
        "\n",
        "We hope this notebook will provide a foundation for marine biologists to adapt this to their own data and uses cases. Through uploading new data to Google Drive and mimicking the folder structure presented in the `SurfPerch Demo with Calling in Our Corals Data` folder we share, this notebook can be applied to novel data with minimal changes. Advanced users may wish to adapt the notebook further, or clone the Perch repository and run this locally on their own hardware."
      ],
      "metadata": {
        "id": "1tXYxvgQHto-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=set-up></a>\n",
        "# Set Up"
      ],
      "metadata": {
        "id": "7YP8wqN20FCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=Introduction></a>\n",
        "## Install the `perch` codebase from GitHub\n",
        "This will take several minutes.\n",
        "\n",
        "**Important:** Click restart the Colab runtime after install completes and proceed."
      ],
      "metadata": {
        "id": "JwBu3wV1AeIg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhD2xd7k-g3Y"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/google-research/perch.git@e27e95344c84601759d4d881289a1c2b53697fa6"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Over time dependancy conflicts may arise on Colab. If this install cell or the import cell below do not succesfully execute after the pip install and restart, check:\n",
        "- The [SurfPerch Github](https://github.com/BenUCL/surfperch/tree/surfperch) for an updated version of this notebook.\n",
        "- The original [NeurIPS Workshop Tutorial](https://colab.research.google.com/drive/1gPBu2fyw6aoT-zxXFk15I2GObfMRNHUq?usp=sharing) for updates to the install or imports.\n",
        "- The [Perch GitHub repository](https://github.com/google-research/perch) for open or closed issues on installs and imports.\n",
        "\n",
        "Alternatively, the Perch repository will contain maintened versions of the notebooks (.ipynb files) needed to rum this workflow. You can download and install Perch to your local machine, instead of Colab, by following instructions in the ReadMe on the Perch GitHub."
      ],
      "metadata": {
        "id": "vt1oRy4mveBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the required imports\n",
        "\n",
        "\n",
        "Beyond the `perch` package pip-installed in the cell above, all other required libraries are already either covered within that installation or already installed in the default Python 3 Google Compute Engine backend/runtime.\n",
        "\n",
        "We note again that any `chirp` modules are provided by the Perch code."
      ],
      "metadata": {
        "id": "6ZaIL2TgAiQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing the datasets\n",
        "\n",
        "We will make use of three datasets. The original data for each totals in the dozens to thousands of hours. For simplicity, here we will work with 15-25 hours from each. Some of the most commonly identified sounds by CIOC have been taken from each and will be used. Many of these sounds are of an unknown orign, most likely reef fish. Each dataset can be divided into two or more ecological categories. We will explore whether acoustic data can be used to identify differences between these ecological categories.\n",
        "\n",
        "### Australia\n",
        "The Australian dataset was gathered from 8 sites around [Lizard Island Research Station](https://maps.app.goo.gl/Kyry7XcgEuxt7jNUA) on the Great Barrier Reef. Here, sites can be divided into high and low fish diversity sites. More details are available in a [previous study](https://doi.org/10.1101/2024.02.02.578582) which used this dataset. **For an easy first pass through this tutorial, try starting with the Ambon Damselfish sound from this dataset** when prompted later. We thank Dr Laura Laura Richardson, University of Bangor, and other co-authors of [Wiliams et al (2024)](https://doi.org/10.1101/2024.02.02.578582) for providing this dataset.\n",
        "\n",
        "### Indonesia\n",
        "The Indonesian dataset was gathered from 18 sites around [HOPE Reef](https://maps.app.goo.gl/fqfyecnXbVgjHi7B9) in Indonesia, pictured below. Hope is the flagship site of the worlds largest coral reef restoration program, you can read more about this intitiative at [buildingcoral.com](buildingcoral.com). We will use recordings taken from healthy, degraded and restored sites.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/BenUCL/surfperch/surfperch/manuscript-additions/colab_pics/hope_reef.jpg\" width=\"50%\">\n",
        "\n",
        "\n",
        "### Philippines\n",
        "This dataset was gathered from 10 sites within the Philippines. These sites were split evenly into protected and unprotected categories. This dataset is detailed further on the Calling in Our Corals platform."
      ],
      "metadata": {
        "id": "HNa-27EJArAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Hosting in Google Drive\n",
        "\n",
        "To facilitate an easy \"plug-and-play\" experience and reduce overhead requirements, we have hosted the data and pretrained SurfPerch model used in a publicly accessible Google Drive folder. Should access ever be lost, you can also find both of these  from the following Zenodo repository and upload them to your Google Drive: (TODO add link)\n",
        "\n",
        "To make sure that your copy of the Colab is connected to the Drive, please create a shortcut to this folder in your MyDrive:\n",
        "- Navigate to the shared data [Google Drive folder](https://drive.google.com/drive/folders/1LjVh15pCeyUs2G163AjRYGFOCVAFi3Hr) (TODO update link)\n",
        "- Click the dropdown menu labeled `SurfPerch Demo with CIOC Data`\n",
        "- Select `Organize` -> `Add shortcut`.\n",
        "- Click the `All locations` tab and then next to `My Drive` click `Add`.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/BenUCL/surfperch/surfperch/manuscript-additions/colab_pics/add_drive_shortcut1.png\" width=\"60%\">\n",
        "<img src=\"https://raw.githubusercontent.com/BenUCL/surfperch/surfperch/manuscript-additions/colab_pics/add_drive_shortcut2.png\" width=\"60%\">\n"
      ],
      "metadata": {
        "id": "UDiPb-EiAs-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Drive to access the files stored in Drive in this Colab\n",
        "\n",
        "Upon executing the cell below, a pop-up should appear that requires authorization to connect Drive. Approve this and then continue below.\n",
        "\n",
        "**IMPORTANT:** This tutorial will write data back in your personal Google Drive.  Please make sure that the `drive_output_directory` variable below does not collide with any existing folders in your Google Drive.\n"
      ],
      "metadata": {
        "id": "SDZRyy0NA0tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The necessary pipeline to connect this Colab environment with the Google Drive\n",
        "# folder where we host the data used in this tutorial.\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# If you followed the above instructions for creating a shortcut to the shared\n",
        "# Drive folder, you should be able to navigate to this directory in the left\n",
        "# hand \"Files\" menu in this Colab (indicated by the Folder icon on the far left\n",
        "# menu).\n",
        "drive_shared_data_folder = '/content/drive/MyDrive/SurfPerch Demo with Calling in Our Corals Data/'\n",
        "\n",
        "# This is the location that this tutorial will use to save data.\n",
        "drive_output_directory = '/content/drive/MyDrive/SurfPerch Demo with Calling in Our Corals Outputs/'\n",
        "if not os.path.exists(drive_output_directory):\n",
        "  os.mkdir(drive_output_directory)"
      ],
      "metadata": {
        "id": "3jILpYswApKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"methodology\"></a>\n",
        "# Methodology üíª"
      ],
      "metadata": {
        "id": "bU6wwY-oDlYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Our method performs a **vector search**: given a **labeled** vocalization which we'll refer to as a **query** (an audio clip with a known species vocalizing), an unlabeled audio dataset which we'll refer to as our **search corpus** (in our case, a soundscape dataset from one the three coral reef datasets), slice up the search corpus into a collection of clips and search over that corpus to find \"matches\" with the query.\n",
        "\n",
        "To do this, we'll follow theses high level steps:\n",
        "\n",
        "**Step 1:** Use the \"off the shelf\" SurfPerch model to generate high quality embeddings for the unlabeled **search corpus** (ie the Indonesian dataset).\n",
        "\n",
        "**Step 2:** Obtain a small number of **query** samples as target sounds, which are labeled samples of unknown fish sounds found by CIOC.\n",
        "\n",
        "**Step 3:** Select a query sample and generate the embedding(s) from this.\n",
        "\n",
        "**Step 4:** Search within the set of embeddings generated from the search corpus (the raw audio data) for points that are \"nearby\" the embedding generated from the query sample. This yields a set of audio snippets from the audio data that should sound similar to the query.\n",
        "\n",
        "**Step 5:** Manually audit the results of step 4. This involves listening to a small number of samples and manually labeling them as a match to our target query or not.\n",
        "\n",
        "--------------------\n",
        "**Note:** Aim to repeat steps 4 and 5 until we have 20-30 samples for our target sound. We are \"bootstrapping\" a training set for a simple linear model.\n",
        "\n",
        "--------------------\n",
        "**Step 6:** Train a simple linear model based on the bootstrapped samples that we just generated.\n",
        "\n",
        "At this point, we have what should be a high quality classifier that can detect a given target sound in its broader dataset. If our model is not performing as well as we'd like, we can continue to generate more training data by repeating the above process using outputs from our linear model to laebl more data, or repeating the original steps 4 and 5 above.\n",
        "\n",
        "\n",
        "\n",
        "This section is broken down into the following parts:\n",
        "*    [Setup and Configuration](#pipeline-config)\n",
        "*    [Generate Embeddings](#embed_data) (Steps 1, 2, and 3)\n",
        "*    [Audio Similarity Search](#similarity-search) (Steps 4 and 5)\n",
        "*    [Train a Linear Classifier](#active-learning) (Step 6)\n",
        "\n"
      ],
      "metadata": {
        "id": "YXXb_6tdDmCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"pipeline-config\"></a>\n",
        "## Setup and Configuration\n",
        "\n",
        "In this section, we set the configuration parameters we'll be using to process and embed the audio data, along with input and output paths for reading in the data, pre-trained model, and writing the results we produce to file."
      ],
      "metadata": {
        "id": "0i7HDOFyDqBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import various dependencies, including the relevant modules from the Perch\n",
        "# repository. Note that \"chirp\" is the old name that the Perch team used, so any\n",
        "# chirp modules imported here were installed as part of the Perch repository in\n",
        "# one of the previous cells.\n",
        "\n",
        "import collections\n",
        "from collections import Counter\n",
        "from etils import epath\n",
        "from IPython.display import HTML\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display as ipy_display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "from ml_collections import config_dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import wavfile\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from chirp.inference import colab_utils\n",
        "colab_utils.initialize(use_tf_gpu=True, disable_warnings=True)\n",
        "\n",
        "# from chirp import audio_utils\n",
        "# from chirp import config_utils\n",
        "# from chirp import path_utils\n",
        "# from chirp.configs import config_globals\n",
        "# from chirp.inference import embed_lib\n",
        "# from chirp.inference import models\n",
        "# from chirp.inference import tf_examples\n",
        "# from chirp.models import metrics\n",
        "# from chirp.projects.bootstrap import bootstrap\n",
        "# from chirp.projects.bootstrap import search\n",
        "# from chirp.projects.bootstrap import display\n",
        "# from chirp.projects.multicluster import classify\n",
        "# from chirp.projects.multicluster import data_lib\n",
        "from chirp import audio_utils\n",
        "from chirp import config_utils\n",
        "from chirp import path_utils\n",
        "from chirp.inference import embed_lib\n",
        "from chirp.inference import models\n",
        "from chirp.inference import tf_examples\n",
        "from chirp.models import metrics\n",
        "from chirp.inference.search import bootstrap\n",
        "from chirp.inference.search import search\n",
        "from chirp.inference.search import display\n",
        "from chirp.inference.classify import classify\n",
        "from chirp.inference.classify import data_lib\n",
        "\n",
        "# If connected to a Colab GPU runtime we should see a GPU listed\n",
        "tf.config.list_physical_devices()"
      ],
      "metadata": {
        "id": "aOif-55bDJHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the configuration to use throughout the tutorial **[do not change]**\n",
        "\n",
        "**To the reader: unless you are adapting this pipeline to your own model or dataset, you can skip the details in this section.**\n",
        "\n",
        "As mentioned above, the Perch codebase provides a general framework for agile modeling, which naturally involves many different configuration options.  We highlight some of the relevant paramers below, and refer the reader to the Perch\n",
        "codebase for more detail.\n",
        "\n",
        "For this tutorial using the provided pre-trained model and datasets, the required values for the following parameters are fixed and should not be changed.\n",
        "\n",
        "**Note** that we will also save the configuration file to support reproducibility (e.g. in the event we wish to work with the embeddings at a later point).\n",
        "\n",
        "#### Relevant Parameters\n",
        "* `window_size_s`: The size in seconds of each \"chunk\" of audio.  Each chunk of audio will be treated like\n",
        " a single data point. Note that the model architecture depends on this value, so once selected it cannot be changed.\n",
        "\n",
        "* `hop_size_s`: The hop size (aka model [*stride*](https://medium.com/machine-learning-algorithms/what-is-stride-in-convolutional-neural-network-e3b4ae9baedb)) is the offset in seconds between successive chunks of audio. When hop_size is equal to window size, the chunks of audio will not overlap at all. Choosing a smaller hop size (a common choice is half of the window_size) may be useful for capturing interesting data points that\n",
        "correspond to audio on the boundary between two windows. However, a smaller\n",
        "hop size may also lead to a larger embedding dataset because each instant of\n",
        "audio is now pesent in multiple windows. As a consequence, you might need to\n",
        "\"de-dupe\" your matches since multiple embedded data points may correspond to the same snippet of raw audio.\n",
        "\n",
        "* `sample_rate`: We use a uniform sample rate of 32 kHz. All audio used for training the base model and generating embeddings is (re)sampled at 32 kHz. This parameter, together with the `window_size_s` of 5 means that each snippet of audio gets represented as a vector of length 5s * 32,000Hz = 160,000.  The value 160,000 must be compatible with your model architecture."
      ],
      "metadata": {
        "id": "bJmA4vwbDu7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model specific parameters: PLEASE DO NOT CHANGE THE CODE IN THIS CELL.\n",
        "config = config_dict.ConfigDict()\n",
        "embed_fn_config = config_dict.ConfigDict()\n",
        "embed_fn_config.model_key = 'taxonomy_model_tf'\n",
        "model_config = config_dict.ConfigDict()\n",
        "\n",
        "# The size of each \"chunk\" of audio.\n",
        "model_config.window_size_s = 5.0\n",
        "\n",
        "# The hop size\n",
        "model_config.hop_size_s = 5.0\n",
        "\n",
        "# All audio in this tutorial is resampled to 32 kHz.\n",
        "model_config.sample_rate = 32000\n",
        "\n",
        "# The location of the pre-trained model.\n",
        "model_config.model_path = drive_shared_data_folder + 'SurfPerch-model/'\n",
        "\n",
        "# Only write embeddings to reduce size. The Perch codebase supports serializing\n",
        "# a variety of metadata along with the embeddings, but for the purposes of this\n",
        "# tutorial we will not need to make use of those features.\n",
        "embed_fn_config.write_embeddings = True\n",
        "embed_fn_config.write_logits = False\n",
        "embed_fn_config.write_separated_audio = False\n",
        "embed_fn_config.write_raw_audio = False\n",
        "\n",
        "config.embed_fn_config = embed_fn_config\n",
        "embed_fn_config.model_config = model_config\n",
        "\n",
        "# These two settings can be used to break large inputs up into smaller chunks;\n",
        "# this is especially helpful for dealing with long files or very large datasets.\n",
        "# Given free colab has limited resources, we set shard_len_s so only 10s of each\n",
        "# file is processed at a time, preventing system RAM from becoming overloaded.\n",
        "config.shard_len_s = 10\n",
        "config.num_shards_per_file = -1\n",
        "\n",
        "# Number of parent directories to include in the filename. This allows us to\n",
        "# process raw audio that lives in multiple directories.\n",
        "config.embed_fn_config.file_id_depth = 1\n",
        "\n",
        "# If your dataset is large its useful to split the TFRecords across multiple\n",
        "# shards so I/O operations can be parallized.\n",
        "config.tf_record_shards = 1"
      ],
      "metadata": {
        "id": "FuODTq6GDs3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify the data (inputs) and results (outputs) directories\n",
        "\n",
        "You do not need to change these values for this tutorial - the values already specified should map to the provided data hosted in Google Drive, and will map to the folder in your local Drive for which you added the shorcut.\n",
        "\n",
        "To execute this pipeline, we need paths to:\n",
        "- `unlabeled_audio_pattern`: the file within Drive where the unlabeled audio dataset is stored.\n",
        "- `embedding_output_dir`: the directory where the embedded audio will be written.\n",
        "- `labeled_data_path`: the directory where the labeled samples will be placed post-search and active learning loop."
      ],
      "metadata": {
        "id": "lT35gAD5EFnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hit run on this cell and choose a dataset\n",
        "# Custom CSS to increase label width\n",
        "style = \"\"\"\n",
        "<style>\n",
        ".widget-label {\n",
        "    min-width: 100px !important;\n",
        "}\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# Apply the CSS\n",
        "ipy_display(HTML(style))\n",
        "\n",
        "# Define the dropdown\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=['Indonesia', 'Australia','Philippines'],\n",
        "    value='Australia',\n",
        "    description='Current choice:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Define a function that reacts to changes in the dropdown\n",
        "def on_dataset_change(change):\n",
        "    dataset_choice = change['new']\n",
        "    print(f'Changed dataset to: {dataset_choice}. Now work through the cells below for this dataset.')\n",
        "\n",
        "# Attach the observer to the dropdown\n",
        "dropdown.observe(on_dataset_change, names='value')\n",
        "\n",
        "# Display the dropdown\n",
        "ipy_display(dropdown)\n",
        "\n"
      ],
      "metadata": {
        "id": "kEy6_Bs7Gcku",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the chosen datasets name\n",
        "dataset_folder = dropdown.value + '/'\n",
        "\n",
        "# Specify a glob pattern matching any number of wave files.\n",
        "# Use [wW][aA][vV] to match .wav or .WAV files\n",
        "unlabeled_audio_pattern = os.path.join(drive_shared_data_folder, dataset_folder, 'raw_audio/*.[wW][aA][vV]')\n",
        "\n",
        "# Specify a directory where the embeddings will be written.\n",
        "embedding_output_dir = os.path.join(drive_output_directory, dataset_folder, 'raw_embeddings/')\n",
        "if not os.path.exists(embedding_output_dir):\n",
        "  os.makedirs(embedding_output_dir, exist_ok=True)\n",
        "\n",
        "config.output_dir = embedding_output_dir\n",
        "config.source_file_patterns = [unlabeled_audio_pattern]\n",
        "\n",
        "# Create output directory and write the configuration.\n",
        "output_dir = epath.Path(config.output_dir)\n",
        "output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Don't forget to run the dropdown cell above!"
      ],
      "metadata": {
        "id": "5yCPzuHaEDjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write the configuration to JSON to ensure consistency with later stages of the pipeline"
      ],
      "metadata": {
        "id": "toUlCrbEMWlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This dumps a config json file next to the embeddings that allows us to reuse\n",
        "# the same embeddings and ensure that we have the correct config that was used\n",
        "# to generate them.\n",
        "embed_lib.maybe_write_config(config, output_dir)\n",
        "\n",
        "# Create SourceInfos configuration, used in sharded computation when computing\n",
        "# embeddings. These source_infos contain metadata about how we're going to\n",
        "# partition the search corpus.  In particular, we're splitting the Powdermill\n",
        "# audio into hundreds of 5s chunks, and the source_infos help us keep track of\n",
        "# which chunk came from which raw audio file.\n",
        "source_infos = embed_lib.create_source_infos(\n",
        "    config.source_file_patterns,\n",
        "    config.num_shards_per_file,\n",
        "    config.shard_len_s)\n",
        "print(f'Constructed {len(source_infos)} source infos.')"
      ],
      "metadata": {
        "id": "BrajR1i9MQ4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the pre-trained embedding model\n",
        "\n",
        "We will apply *transfer learning* by using the pretrained SurfPerch to compute the embeddings we will search over. This pretrained model allows us to leverage a rich learned representation so that we do not need to train a custom model for the specific species we search for."
      ],
      "metadata": {
        "id": "xSeH6yjgMq5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { vertical-output: true }\n",
        "# Here we're loading our generic Bird Classifier model.\n",
        "# The embed_fn object is a wrapper over the model.\n",
        "embed_fn = embed_lib.EmbedFn(**config.embed_fn_config)\n",
        "print('\\n\\nLoading model(s)...')\n",
        "embed_fn.setup()\n",
        "\n",
        "print('\\n\\nTest-run of model...')\n",
        "z = np.zeros([int(model_config.sample_rate * model_config.window_size_s)])\n",
        "embed_fn.embedding_model.embed(z)\n",
        "print('Setup complete!')"
      ],
      "metadata": {
        "id": "Cre7IjmKMqd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=embed_data></a>\n",
        "## Generate Embeddings ‚è≥\n",
        "\n",
        "In this section we'll generate the **embeddings** corresponding to both our search corpus (the chosen dataset) as well as our query audio (the target sound chosen shortly).\n",
        "\n",
        "Recall that the embeddings are new representations of the original data that are generated by the pretrained model. It is important to remember that we are **not** using our pretrained model to classify the new reef dataset.  Rather, we're using the model's **learned features** to map our data into a new representation that is more amenable to simpler classification techniques. The pretrained model was very computationally costly to train. So, the idea is that the heavy lifting of learning salient features has already been done during development of the pretrained model. We can use this pretrained model to extract these features from new marine bioacoustic data, then train a much lighter-weight machine learning model on top of these features. This is the concept of **transfer learning**, ie, re-using the features learned by a model, but in a novel setting.\n"
      ],
      "metadata": {
        "id": "-hQdBjvDM5Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process the search dataset"
      ],
      "metadata": {
        "id": "qRxc_C0vM7xU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To reduce the overhead computational resources required and speed up execution\n",
        "# time, we use multiple threads to load the audio before embedding. This tends\n",
        "# to perform faster, but can fail if any audio files are corrupt.\n",
        "\n",
        "# The source_infos variable contains metadata about how to parition the search\n",
        "# corpus.  This step creates an audio_iterator which iterates over the 5 second\n",
        "# chunks of audio.\n",
        "embed_fn.min_audio_s = 1.0\n",
        "record_file = (output_dir / 'embeddings.tfrecord').as_posix()\n",
        "succ, fail = 0, 0\n",
        "\n",
        "audio_iterator = audio_utils.multi_load_audio_window(\n",
        "    filepaths=[s.filepath for s in source_infos],\n",
        "    offsets=[s.shard_num * s.shard_len_s for s in source_infos],\n",
        "    sample_rate=model_config.sample_rate,\n",
        "    window_size_s=config.shard_len_s,\n",
        ")"
      ],
      "metadata": {
        "id": "q9g-HXCLM7lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embed the search dataset\n",
        "We are ready to generate the embeddings for the raw audio.  This cell iterates over the `audio_iterator` created in the previous cell and creates a point (vector) in *embedding space* for each 5 second chunk of raw audio.  We write these embeddings to files (which are written into your `emebdding_output_dir` directory in GDrive that we specified above), and then return a `ds` variable that is a handle on the resulting TFRecordDataset object.\n",
        "\n",
        "Writing the embeddings to file is useful because for large datasets, this embedding step can take minutes or hours, and we don't want to have to repeatedly regenerate the embeddings.\n",
        "\n",
        "**GPU usage**: This component will benefit greatly from using a GPU, which can be selected from the runtime menu. Free GPU usage is limited, you can reduce your usage by first running the notebook up to here on a GPU. You can then switch to a CPU runtime and start over, allowing you to skip past this cell and use the precomputed embeddings now in your GDrive.\n",
        "\n",
        "‚ùóIf you have already computed the embeddings for this dataset, you do not need to run this cell again‚ùó"
      ],
      "metadata": {
        "id": "QjcRidvpNBQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { vertical-output: true }\n",
        "# Embed! This step may take several minutes to run.\n",
        "with tf_examples.EmbeddingsTFRecordMultiWriter(\n",
        "    output_dir=output_dir, num_files=config.tf_record_shards) as file_writer:\n",
        "  for source_info, audio in tqdm.tqdm(\n",
        "      zip(source_infos, audio_iterator), total=len(source_infos)):\n",
        "    if audio.shape[0] < embed_fn.min_audio_s * model_config.sample_rate:\n",
        "      # Ignore short audio.\n",
        "      continue\n",
        "    file_id = source_info.file_id(config.embed_fn_config.file_id_depth)\n",
        "    offset_s = source_info.shard_num * source_info.shard_len_s\n",
        "    example = embed_fn.audio_to_example(file_id, offset_s, audio)\n",
        "    if example is None:\n",
        "      fail += 1\n",
        "      continue\n",
        "    file_writer.write(example.SerializeToString())\n",
        "    succ += 1\n",
        "  file_writer.flush()\n",
        "print(f'\\n\\nSuccessfully processed {succ} source_infos, failed {fail} times.')\n",
        "\n",
        "fns = [fn for fn in output_dir.glob('embeddings-*')]\n",
        "ds = tf.data.TFRecordDataset(fns)\n",
        "parser = tf_examples.get_example_parser()\n",
        "ds = ds.map(parser)\n",
        "for ex in ds.as_numpy_iterator():\n",
        "  print('Recording filename:', ex['filename'])\n",
        "  print('Shape of the embedding:', ex['embedding'].shape)\n",
        "  break\n",
        "\n",
        "# This can take a few moments to get started"
      ],
      "metadata": {
        "id": "L8_CBc_QMYy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=similarity-search></a>\n",
        "## Audio Similarity Search üîé\n",
        "\n",
        "In the previous section we generated a set of embeddings for our search corpus as well as for our target quert.  In this section we will 'search' within the search corpus to find examples that are similar to our query, and we'll manually label each sample as our target query or 'Unknown'.  These labeled samples then get saved and will be used as training data for the linear classifier in the next section.\n",
        "\n",
        "Recall that an *embedding* is simply a vector (point) in some high dimensional space.  The similarity search we implement relies on Euclidean distance between two vectors. Other metrics can be used to compare two vectors, such as cosine similarity or Maximum Inner Product (MIP).\n",
        "\n",
        "**Note:** In practice, we end up looping through this section repeatedly with different query samples (if we have them) and different search parameters.  That helps us generate a robust training set.\n"
      ],
      "metadata": {
        "id": "Kf3VZFiQOUH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the target classes\n",
        "\n",
        "We specify the classes (aka labels) that we want to search for within the search corpus. In our case, we're primariy interested in finding instances of our target query, so this will be one of our two classes alongside an \"Unknown\" class, where \"Unknown\" will be used for anything that is not the target sample\n",
        "\n",
        "We note here that these classes are completely arbitrary. The Active Learning technique is *extremely* flexible and can be adapted to a wide variety of use cases. For example, the Perch team recently applied this technique to identify specific types of vocalizations for a single target species. In that case, we simply added the classes of interest to this `target_classes` list."
      ],
      "metadata": {
        "id": "Iwiry7fpOWQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets view the target sounds CIOC revealed\n",
        "\n",
        "A selection of sounds revealed by CIOC were chosen. Here we can view the sounds for your respective dataset."
      ],
      "metadata": {
        "id": "cgDZNJvM9VKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { vertical-output: true }\n",
        "# Path to cioc target sound folders\n",
        "cioc_sounds = drive_shared_data_folder + dataset_folder + 'cioc_sounds'\n",
        "cioc_sounds_folders = os.listdir(cioc_sounds)\n",
        "\n",
        "# For each target sound folder, find the first audio file as an example\n",
        "example_target_sounds = []\n",
        "for folder in cioc_sounds_folders:\n",
        "  wav_files = [file for file in os.listdir(os.path.join(cioc_sounds, folder)) if file.lower().endswith('.wav')]\n",
        "  example_sound_path = os.path.join(cioc_sounds, folder + '/' + wav_files[0])\n",
        "  example_target_sounds.append(example_sound_path)\n",
        "\n",
        "# Now view each example target sound\n",
        "print('Number of different target sounds: ', len(example_target_sounds))\n",
        "for audio_path in example_target_sounds:\n",
        "  print('Target sound label: ', audio_path.split('/')[-2])\n",
        "  audio = audio_utils.load_audio(audio_path, model_config.sample_rate)\n",
        "  display.plot_audio_melspec(audio, model_config.sample_rate)"
      ],
      "metadata": {
        "id": "F3yneFq3-28r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hit run on this cell and pick a target sound\n",
        "\n",
        "# Ensure the path exists and list directories\n",
        "if os.path.exists(cioc_sounds):\n",
        "    sound_folders = [f for f in os.listdir(cioc_sounds) if os.path.isdir(os.path.join(cioc_sounds, f))]\n",
        "else:\n",
        "    print(\"Path does not exist:\", cioc_sounds)\n",
        "    sound_folders = []\n",
        "\n",
        "# Create and display the dropdown\n",
        "sound_dropdown = widgets.Dropdown(\n",
        "    options=sound_folders,\n",
        "    description='Select sound:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Define a function that reacts to changes in the dropdown\n",
        "def on_sound_change(change):\n",
        "    choice = change['new']\n",
        "    print(f'Changed target sound to: {choice}. Now work through the cells below for this dataset.')\n",
        "\n",
        "# Attach the observer to the dropdown\n",
        "sound_dropdown.observe(on_sound_change, names='value')\n",
        "\n",
        "ipy_display(sound_dropdown)"
      ],
      "metadata": {
        "id": "hOWPFRfyNC7f",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the query vector\n",
        "\n",
        "We're now ready to create the 'query', which uses our selected target sound.  In general, this query is the audio that you're looking for in your search corpus."
      ],
      "metadata": {
        "id": "vLE0yH6hv86w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load and view the query audio sample { vertical-output: true }\n",
        "target_sound = sound_dropdown.value\n",
        "target_classes = [target_sound, 'Unknown']\n",
        "\n",
        "# Select one of the target audio files. Default with 1, but for many sounds\n",
        "# CIOC users surfaced multiple copies which we can leverage. The print\n",
        "# out from this cell will tell you if there are others to choose from.\n",
        "file_index = 1  #@param\n",
        "file_index = file_index -1\n",
        "\n",
        "# Build the folder path\n",
        "target_audio_folder = os.path.join(drive_shared_data_folder, dataset_folder, 'cioc_sounds', target_sound)\n",
        "\n",
        "# Retrieve all .wav files\n",
        "wav_files = [file for file in os.listdir(target_audio_folder) if file.lower().endswith('.wav')]\n",
        "\n",
        "# Print the total number of audio files\n",
        "print(f\"Number of indexed audio files in target sound directory: {len(wav_files)}\")\n",
        "\n",
        "# Validate the user input and select the audio file\n",
        "if 0 <= file_index < len(wav_files):\n",
        "    audio_path = os.path.join(target_audio_folder, wav_files[file_index])\n",
        "    print(f\"Viewing example: {file_index + 1}\")\n",
        "else:\n",
        "    print(\"Invalid file index. Please select a valid index up to and including: \", len(wav_files))\n",
        "    audio_path = None\n",
        "\n",
        "# Assuming the rest of the code executes only if a valid path is selected\n",
        "if audio_path:\n",
        "    audio = audio_utils.load_audio(audio_path, model_config.sample_rate)\n",
        "    display.plot_audio_melspec(audio, model_config.sample_rate)\n"
      ],
      "metadata": {
        "id": "yMXshzfYqvi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Select the specific query window\n",
        "\n",
        "All our target sounds are 5s long, which is the input window length for the SurfPerch model. If using your own data you may have target sound files longer than 5s. You can play with the start_s variable to pick the starting point you wish to use from your longer file.\n",
        "\n",
        "Note, its usually best to use a minimum sample length of 5s. Silence will be added after the target sound to total 5s otherwise, which doesn't always perform as well when performing the search later on."
      ],
      "metadata": {
        "id": "jiQOhQHZ2y8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { vertical-output: true }\n",
        "# If you're audio clip is longer than 5s, adjust start_s to pick your\n",
        "# prefered start time.\n",
        "start_s = 0  #@param\n",
        "\n",
        "# Display the selected window.\n",
        "print('Selected audio window:')\n",
        "st = int(start_s * model_config.sample_rate)\n",
        "end = int(st + model_config.window_size_s * model_config.sample_rate)\n",
        "if end > audio.shape[0]:\n",
        "  end = audio.shape[0]\n",
        "  st = max([0, int(end - model_config.window_size_s * model_config.sample_rate)])\n",
        "audio_window = audio[st:end]\n",
        "display.plot_audio_melspec(audio_window, model_config.sample_rate)\n",
        "\n",
        "query_audio = audio_window\n",
        "sep_outputs = None"
      ],
      "metadata": {
        "id": "Zg_wCwWd4krj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Copy CIOC samples to our labeled data directoy\n",
        "As seen above, CIOC was able to reveal multiple examples for some target sounds. These provide highly useful replicates which can be used to boost the training data for our model later on. Here, we will move them to the `labeled_data_path`, which is the folder where the additional samples we find during the search phase later will be added.\n",
        "\n",
        "The search phase may well find some of the same samples. The code will make sure not to save duplicate copies.\n"
      ],
      "metadata": {
        "id": "e8esSAXZtSIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The path to an empty directory where the generated labeled samples will be\n",
        "# placed. Each labeled sample will be placed into a subdirectory corresponding\n",
        "# to the target class that we select for that sample.\n",
        "target_audio_outputs = os.path.join(drive_output_directory, dataset_folder, target_sound + '/labeled_outputs/' + target_sound)\n",
        "os.makedirs(target_audio_outputs, exist_ok=True)\n",
        "\n",
        "# Copy all .wav and .WAV files from target_audio_folder to labeled_data_path\n",
        "for file in os.listdir(target_audio_folder):\n",
        "    if file.lower().endswith('.wav'):\n",
        "        source_path = os.path.join(target_audio_folder, file)\n",
        "        destination_path = os.path.join(target_audio_outputs, file)\n",
        "        shutil.copy2(source_path, destination_path)"
      ],
      "metadata": {
        "id": "iOGt_qh7tcaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embed the query\n",
        "\n",
        "Here we run the query audio through the embedding function to generate the embedding vector for the query."
      ],
      "metadata": {
        "id": "CpquNlk_5bHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = query_audio\n",
        "\n",
        "embedded_query = embed_fn.embedding_model.embed(query).embeddings[ :, 0, :]"
      ],
      "metadata": {
        "id": "nxZFKYRy5cH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a TensorFlow Dataset (TFDS) wrapper over the embeddings\n",
        "\n",
        "This is a technical step that wraps our search corpus in a TFDS object to allow us to use some convenient built-in features."
      ],
      "metadata": {
        "id": "yS5uyFYr5qJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the embedded dataset that we created above...\n",
        "bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_config(\n",
        "    embeddings_path=embedding_output_dir,\n",
        "    annotated_path=target_audio_outputs\n",
        ")\n",
        "\n",
        "project_state = bootstrap.BootstrapState(\n",
        "    bootstrap_config, embedding_model=embed_fn.embedding_model)\n",
        "\n",
        "embeddings_ds = project_state.create_embeddings_dataset()"
      ],
      "metadata": {
        "id": "ZUlLsXPJ5dqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=top-k-search></a>Run top-k search using a comparison metric\n",
        "\n",
        "### <a name=top-k-search></a>Run top-k search using a comparison metric\n",
        "\n",
        "In this cell we run a \"nearest neighbor\" search over the search corpus to find embeddings that are closest to the query embedding according to our chosen metric.  These will correspond to snippets of our chosen dataset that sound similar to our query.\n",
        "\n",
        "The `target_score` variable is a parameter that allows us to surface not just the closest matches, but rather matches that lie some fixed distance away.  When `target_score` is set to `None` (or `0`), the search will return the closest matches.\n",
        "\n",
        "Recall that our goal in this section is to generate training data for a linear classifier. In order to train a robust model, we want this training dataset to contain both obvious/easy examples as well as not-so-obvious examples. If in our search we just looked for the closest possible matches, we would likely only find easy examples. The `target_score` param allows us to look for examples that might be less obvious (because they are farther away in embedding space).\n",
        "\n",
        "Additionally, do not underestimate the usefulness of labeling 'Unknown' negative samples. Labeling a broad representation of negative samples present in the full dataset, especially those that currently score as close matches in the search, is key for training a classifier robust to false positives.\n",
        "\n",
        "**Important note:** You will likely need to come back to this cell and produce more top-k instances by modulating the value of `target_score`. We explain the approach to choosing useful values for `target_score` in the section [Choose a target score](#choosing-target-score)."
      ],
      "metadata": {
        "id": "XMaYgTOn6Q84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of search results to capture. top_k = 25 is often a good start,\n",
        "# but we use 10 for brevity in this demo.\n",
        "top_k = 10 # @param {type:\"number\"}\n",
        "\n",
        "# The Perch codebase supports:\n",
        "#  'euclidean', which is the standard euclidean distance\n",
        "#  'cosine', which is the cosine similarity,\n",
        "#  'mip', which is Maximum Inner Product\n",
        "metric = 'euclidean'  #@param['euclidean', 'mip', 'cosine']\n",
        "\n",
        "# Target distance for search results. This lets us try to hone in on a\n",
        "# 'classifier boundary' instead of just looking at the closest matches.\n",
        "# Set to 'None' for raw 'best results' search.\n",
        "target_score = None #@param\n",
        "\n",
        "results, all_scores = search.search_embeddings_parallel(\n",
        "    embeddings_ds, embedded_query,\n",
        "    hop_size_s=model_config.hop_size_s,\n",
        "    top_k=top_k, target_score=target_score, score_fn=metric,\n",
        "    random_sample=False)"
      ],
      "metadata": {
        "id": "YlDgOC_T6Jdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User-in-the-loop data labeling (**requires user interaction**)\n",
        "\n",
        "The cell below displays the search results in a user-interface in the following format:\n",
        "* Image: a plot visualization of the audio search result (Mel spectrogram, frequency in Hz over time)\n",
        "* A playback of the audio sample itself\n",
        "* Metrics and metadata: `rank` position, `source file` of the recording segment, `offset_s` (in seconds) from the recording, and the search `score` (i.e. similarity with the query)\n",
        "* Candidate labels for the sample\n",
        "\n",
        "**Instructions to the user:** <br>\n",
        "For each search result presented below, select either the target label or Unknown class. Samples which present difficult cases can be highly valuable to label, but if any cannot be labeled with confidence then its possible to leave them blank.\n",
        "\n",
        "**Quick guide on how to verify vocalisations:**\n",
        "\n",
        "One of the hardest parts of the entire process can be learning to correctly identify the target sounds for yourself. Often this requires a skilled bioacoustician to do well. Many common reef sounds can blur from one clear signal into another, especially tricky pulse, purr and pop vocalizations. When in doubt, take the time to check back against your original target sound. Make use of listening with a good set of earphones or headphones and also visually assessing the spectrogram.\n",
        "\n",
        "For an easy start try the whoop of the Ambon Damselfish in the Australian dataset!"
      ],
      "metadata": {
        "id": "GgNyAbaV6joY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display the search results for user labeling. { vertical-output: true }\n",
        "display.display_search_results(\n",
        "    results, model_config.sample_rate, project_state.source_map,\n",
        "    checkbox_labels=target_classes,\n",
        "    max_workers=5)\n",
        "\n",
        "# Let this cell finish executing before labeling the samples"
      ],
      "metadata": {
        "id": "vbybOijd6fdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose a target score by plotting a histogram of all distances in the search corpus\n",
        "\n",
        "During the top-k search step above, the Perch code also computed and saved the distances to *every* point in the search corpus. The actual numerical values of these distances are hard to interpret, but the relative values are very useful. In the following cell we plot a histogram of this set of distances to help us conceptualize the geometry of our embedded dataset. This histogram helps us find and tune our values for the `target_score` variable in the top-k search.\n",
        "\n",
        "A typical histogram will appear to fit some vaguely-normal looking distribution, possibly skewed left with a heavy tail. While there is no prescriptive formula for finding useful values of `target_score`, the Perch team has found that good choices tend to lie near the left-hand 'hockey-stick' point of the distribution. For example, in the following histogram, you might try playing with values somewhere in the range of 2.8 to 3.1:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1bLc2XDTqutihg4wJSkCfB2DiT4Dpr4UO\" width=\"40%\" height=\"40%\">\n",
        "\n",
        "These tend to correspond to examples that are faint, or have background noise, or are otherwise not especially obvious. Annotating these examples and adding them to the training set is very important because they help the linear model discriminate better on these less-clear \"boundary\" points. Though take care to make sure your annotations are correct, if in doubt you can leave samples unlabeled.\n",
        "\n"
      ],
      "metadata": {
        "id": "Md568cA0KhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { vertical-output: true }\n",
        "# Plot histogram of distances.\n",
        "ys, _, _ = plt.hist(all_scores, bins=128, density=True)\n",
        "hit_scores = [r.score for r in results.search_results]\n",
        "plt.scatter(hit_scores, np.zeros_like(hit_scores), marker='|',\n",
        "            color='r', alpha=0.5)\n",
        "\n",
        "plt.xlabel(metric)\n",
        "plt.ylabel('density')\n",
        "if target_score is not None:\n",
        "  plt.plot([target_score, target_score], [0.0, np.max(ys)], 'r:')\n",
        "  # Compute the proportion of scores < target_score.\n",
        "  hit_percentage = (all_scores < target_score).mean()\n",
        "  print(f'score < target_score percentage : {hit_percentage:5.3f}')\n",
        "min_score = np.min(all_scores)\n",
        "plt.plot([min_score, min_score], [0.0, np.max(ys)], 'g:')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eWsRs5vxJdma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write the user-annotated search results to file\n",
        "\n",
        "This cell saves the annotations you generated in the previous step.  It writes data to the `labeled_data_path` location that was specified above."
      ],
      "metadata": {
        "id": "QkIH1DpVJbI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_labeled_data(search_results, labeled_data_path: str, sample_rate: int):\n",
        "  \"\"\"Write labeled results to the labeled data collection.\"\"\"\n",
        "  labeled_data_path = epath.Path(labeled_data_path)\n",
        "  counts = collections.defaultdict(int)\n",
        "  duplicates = collections.defaultdict(int)\n",
        "  for r in search_results:\n",
        "    labels = [ch.description for ch in r.label_widgets if ch.value]\n",
        "    if not labels:\n",
        "      continue\n",
        "    extension = epath.Path(r.filename).suffix\n",
        "    filename = epath.Path(r.filename).name[: -len(extension)]\n",
        "    output_filename = f'{filename}___{r.timestamp_offset}{extension}'\n",
        "    for label in labels:\n",
        "      output_path = labeled_data_path / label\n",
        "      output_path.mkdir(parents=True, exist_ok=True)\n",
        "      output_filepath = epath.Path(output_path / output_filename)\n",
        "      if output_filepath.exists():\n",
        "        duplicates[f'{label}'] += 1\n",
        "        continue\n",
        "      else:\n",
        "        counts[label] += 1\n",
        "      with output_filepath.open('wb') as f:\n",
        "        wavfile.write(f, sample_rate, np.float32(r.audio))\n",
        "  for label, count in counts.items():\n",
        "    print(f'Wrote {count} examples for label {label}')\n",
        "  for label, count in duplicates.items():\n",
        "    print(f'Not saving {count} duplicates for label {label}')\n",
        "\n",
        "labeled_data_path = os.path.join(drive_output_directory, dataset_folder, target_sound + '/labeled_outputs/')\n",
        "write_labeled_data(results, labeled_data_path, model_config.sample_rate)"
      ],
      "metadata": {
        "id": "KOUV3uGc6rQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=active-learning></a>\n",
        "## Train a Linear Classifier with active learning ü§ñüß†\n",
        "\n",
        "In the last stage, we labeled samples from the search dataset as matches (by similarity comparison with the queries) for each of our target classes. We will now train a simple linear model using those bootstrapped (labeled) data points from the search dataset.\n",
        "\n",
        "**Important:** in order to be able to train the linear model, we need several examples from each of the two classes (more is better!). If you encounter an error in this section when training the model, you likely did not generate a sufficient amount of labeled data. Please go back to the [Top-k Search](#top-k-search) section and choose a new value of the `target_score` attribute and label some more data."
      ],
      "metadata": {
        "id": "gGYFFAlQJu0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load and embed the search-annotated dataset { vertical-output: true }\n",
        "\n",
        "# Load the training data that is located in the `labeled_data_path` directory.\n",
        "# In that directory there will be folders corresponding to our target labels\n",
        "\n",
        "merged = data_lib.MergedDataset.from_folder_of_folders(\n",
        "    base_dir=labeled_data_path,\n",
        "    embedding_model=project_state.embedding_model,\n",
        "    time_pooling='mean',\n",
        "    load_audio=False,\n",
        "    target_sample_rate=-2,\n",
        "    audio_file_pattern='*',\n",
        "    embedding_config_hash=bootstrap_config.embedding_config_hash(),\n",
        ")\n",
        "\n",
        "# Label distribution\n",
        "lbl_counts = np.sum(merged.data['label_hot'], axis=0)\n",
        "print('num classes :', (lbl_counts > 0).sum())\n",
        "print('mean ex / class :', lbl_counts.sum() / (lbl_counts > 0).sum())\n",
        "print('min ex / class :', (lbl_counts + (lbl_counts == 0) * 1e6).min())"
      ],
      "metadata": {
        "id": "NynZXbxBJsWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a simple linear model using the labeled embeddings\n",
        "\n",
        "### Train a simple linear model using the labeled embeddings\n",
        "\n",
        "We use the following hyperparameters which should serve as reasonably performing defaults to train this linear model (classifier):\n",
        "\n",
        "- `batch_size`: 12\n",
        "- `num_epochs`: 128\n",
        "- `num_hiddens`: -1 (to match the dimensions of the embeddings)\n",
        "- `learning_rate`: 0.001\n",
        "\n",
        "Additionally, we compute the following metrics to measure the \"goodness\" of the trained model:\n",
        "- `acc`: overall accuracy\n",
        "- `auc_roc`: AUC ROC, or area under the receiving curve\n",
        "- `cmAP`: mean average precision averaged across species\n",
        "- `maps`: mean average precision for each class"
      ],
      "metadata": {
        "id": "FyyVgqu0KMgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of random training examples to choose from each class.\n",
        "\n",
        "# Note that if you don't have very many samples you'll need to set\n",
        "# train_ratio=None and train_examples_per_class to a value that is\n",
        "# less than the minimum number of examples you have of each class.\n",
        "\n",
        "# Set exactly one of train_ratio and train_examples_per_class\n",
        "train_ratio = 0.8  #@param\n",
        "train_examples_per_class = None  #@param\n",
        "\n",
        "# Number of random re-trainings. In other words, this value indicates how many\n",
        "# models we will train, each will use a new randomly selected combination of\n",
        "# our labeled samples for training and testing. By training multiple models,\n",
        "# we get a sense of model robustness. Training a single model (ie, num_seeds = 1)\n",
        "# is sufficient for this tutorial, but feel free to bump this up to get a more\n",
        "# reliable estimate.\n",
        "num_seeds = 1  #@param\n",
        "\n",
        "# Classifier training hyperparams.\n",
        "# These should be good defaults.\n",
        "batch_size = 12\n",
        "num_epochs = 128\n",
        "num_hiddens = -1\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "pduqlrkYJkoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell trains the linear model(s) and outputs some summary statistics for\n",
        "# each model. If you only have num_seeds = 1 then we'll only train a single\n",
        "# model here.\n",
        "metrics = collections.defaultdict(list)\n",
        "for seed in tqdm.tqdm(range(num_seeds)):\n",
        "  if num_hiddens > 0:\n",
        "    model = classify.get_two_layer_model(\n",
        "        num_hiddens, merged.embedding_dim, merged.num_classes)\n",
        "  else:\n",
        "    model = classify.get_linear_model(\n",
        "        merged.embedding_dim, merged.num_classes)\n",
        "  run_metrics = classify.train_embedding_model(\n",
        "      model, merged, train_ratio, train_examples_per_class,\n",
        "      num_epochs, seed, batch_size, learning_rate)\n",
        "  metrics['acc'].append(run_metrics.top1_accuracy)\n",
        "  metrics['auc_roc'].append(run_metrics.auc_roc)\n",
        "  metrics['cmap'].append(run_metrics.cmap_value)\n",
        "  metrics['maps'].append(run_metrics.class_maps)\n",
        "  metrics['test_logits'].append(run_metrics.test_logits)"
      ],
      "metadata": {
        "id": "BXs6LxK_KOi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute the average metrics and print the model performance\n",
        "\n",
        "In the previous cells, the `num_seeds` param controls how many times we train a model.  Each time we train a model there is some randomness in terms of which data points we choose from our labeled data, as well as some randomness in the model's initialization.  We can get a sense of how robust our classifier is by training multple times and looking at the summary statistics computed by the following cell.  A low `auc_roc` value (ie, less than 0.9 or so) probably indicates that we should generate some more training data.\n",
        "\n",
        "**Note**: If you are new to labeling marine sounds, it can also be tricky to be sure you've labeled everything correctly."
      ],
      "metadata": {
        "id": "EbGZ_3lUMBpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_acc = np.mean(metrics['acc'])\n",
        "mean_auc = np.mean(metrics['auc_roc'])\n",
        "mean_cmap = np.mean(metrics['cmap'])\n",
        "# Merge the test_logits into a single array.\n",
        "test_logits = {\n",
        "    k: np.concatenate([logits[k] for logits in metrics['test_logits']])\n",
        "    for k in metrics['test_logits'][0].keys()\n",
        "}\n",
        "\n",
        "print(f'acc:{mean_acc:5.2f}, auc_roc:{mean_auc:5.2f}, cmap:{mean_cmap}')\n",
        "#p:5.2f}')}\n",
        "for lbl, auc in zip(merged.labels, run_metrics.class_maps):\n",
        "  if np.isnan(auc):\n",
        "    continue\n",
        "  print(f'\\n{lbl:8s}, auc_roc:{auc:5.2f}')\n",
        "  colab_utils.prstats(f'test_logits({lbl})',\n",
        "                      test_logits[merged.labels.index(lbl)])"
      ],
      "metadata": {
        "id": "ANJXkthNKQUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Active Learning Loop: Generating more training examples\n"
      ],
      "metadata": {
        "id": "9pjXY5z8KxfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate new samples using logit scores\n"
      ],
      "metadata": {
        "id": "lD9u_c-uKzlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the target class to work with.  This must be one of the values from\n",
        "# the target_classes list above (target_sound or 'Unknown')\n",
        "\n",
        "target_class = target_sound  #@param\n",
        "\n",
        "# Choose a target logit. You can start by setting this to 'None' to get the\n",
        "# highest-logit examples, which should reveal more of the target sound.\n",
        "# Next, selecting 0.0 or the hockey stick bend should reveal samples the model\n",
        "# currently finds difficult - these are very valuable to label.\n",
        "target_logit = None  #@param\n",
        "\n",
        "# Number of results to display.\n",
        "num_results = 10  #@param\n",
        "\n",
        "# Create the embeddings dataset.\n",
        "target_class_idx = merged.labels.index(target_class)\n",
        "results, all_logits = search.classifer_search_embeddings_parallel(\n",
        "    embeddings_classifier=model,\n",
        "    target_index=target_class_idx,\n",
        "    embeddings_dataset=embeddings_ds,\n",
        "    hop_size_s=model_config.hop_size_s,\n",
        "    target_score=target_logit,\n",
        "    top_k=num_results\n",
        ")"
      ],
      "metadata": {
        "id": "7w5UIaDRKUWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { vertical-output: true }\n",
        "# Plot the histogram of model logits.\n",
        "_, ys, _ = plt.hist(all_logits, bins=128, density=True)\n",
        "plt.xlabel(f'{target_class} logit')\n",
        "plt.ylabel('density')\n",
        "# plt.yscale('log')\n",
        "plt.plot([target_logit, target_logit], [0.0, np.max(ys)], 'r:')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lVd2rzs2LRaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display results for the target label { vertical-output: true }\n",
        "\n",
        "display_labels = merged.labels\n",
        "\n",
        "extra_labels = []  #@param\n",
        "for label in extra_labels:\n",
        "  if label not in merged.labels:\n",
        "    display_labels += (label,)\n",
        "if 'Unknown' not in merged.labels:\n",
        "  display_labels += ('Unknown',)\n",
        "\n",
        "display.display_search_results(\n",
        "    results, model_config.sample_rate,\n",
        "    project_state.source_map,\n",
        "    checkbox_labels=display_labels,\n",
        "    max_workers=5)"
      ],
      "metadata": {
        "id": "f3HLbQg5NXQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add selected results to the labeled data\n",
        "\n",
        "As before, once we've annotated the examples from the previous cell, we'll save them in the `labeled_data_dir`."
      ],
      "metadata": {
        "id": "8S5vg6tcTOS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.write_labeled_data(labeled_data_path, model_config.sample_rate)"
      ],
      "metadata": {
        "id": "cZxDH_WgNamK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write the trained model's classification results (inference) to CSV\n",
        "\n",
        "Usually the purpose of creating a model in the first place is to bulk-process many hours of raw audio.  In this cell, we'll run our linear model over the entire search corpus.  The output will be a CSV containing the results.  \n",
        "\n",
        "The `threshold` parameter is the minimum logit score that will get recorded (ie, samples with low logit scores are simply omitted from the results).  You can tune this score to generate different output CSVs at different confidence scores. Alternatively, if many periods are omitted but you don't want to introduce potential error by lowering the logit threshold, you can increase the strength of logit scores by carefully labeling more data and retraining your classifier using the active learning process."
      ],
      "metadata": {
        "id": "WCqL-th1Vqdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# { vertical-output: true }\n",
        "\n",
        "threshold = 1.0  #@param\n",
        "output_filepath = os.path.join(drive_output_directory, dataset_folder, target_sound + '/inference_'+ target_sound + '.csv')\n",
        "\n",
        "def classify_batch(batch):\n",
        "  \"\"\"Classify a batch of embeddings.\"\"\"\n",
        "  emb = batch[tf_examples.EMBEDDING]\n",
        "  emb_shape = tf.shape(emb)\n",
        "  flat_emb = tf.reshape(emb, [-1, emb_shape[-1]])\n",
        "  logits = model(flat_emb)\n",
        "  logits = tf.reshape(\n",
        "      logits, [emb_shape[0], emb_shape[1], tf.shape(logits)[-1]])\n",
        "  # Take the maximum logit over channels.\n",
        "  logits = tf.reduce_max(logits, axis=-2)\n",
        "  batch['logits'] = logits\n",
        "  return batch\n",
        "\n",
        "\n",
        "inference_ds = embeddings_ds.map(\n",
        "    classify_batch, num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "all_embeddings = []\n",
        "all_logits = []\n",
        "\n",
        "with open(output_filepath, 'w') as f:\n",
        "  # Write column headers.\n",
        "  headers = ['filename', 'timestamp_s', 'label', 'logit']\n",
        "  f.write(', '.join(headers) + '\\n')\n",
        "  for ex in tqdm.tqdm(inference_ds.as_numpy_iterator()):\n",
        "    all_embeddings.append(ex['embedding'])\n",
        "    all_logits.append(ex['logits'])\n",
        "    for t in range(ex['logits'].shape[0]):\n",
        "      for i, label in enumerate(merged.labels):\n",
        "        if ex['logits'][t, i] > threshold:\n",
        "          offset = ex['timestamp_s'] + t * model_config.hop_size_s\n",
        "\n",
        "          logit = '{:.2f}'.format(ex['logits'][t, i])\n",
        "          row = [ex['filename'].decode('utf-8'),\n",
        "                 '{:.2f}'.format(offset),\n",
        "                 label, logit]\n",
        "          f.write(', '.join(row) + '\\n')\n",
        "\n",
        "all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "all_logits = np.concatenate(all_logits, axis=0)\n",
        "print('Saved results to: ', output_filepath)"
      ],
      "metadata": {
        "id": "4KYXh9JATNOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=results></a>\n",
        "# Results üìä\n",
        "\n",
        "For example datasets used in this demo, we can also compare the occurence of target sounds between the relevant habitat types the data was gathered from.\n",
        "\n",
        "The csv results file contains a list of all filenames and their predicted label. Below is some custom code for the datasets used in this demo which reads the filenames present in the csv and produces plots.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A6tAwjidmtzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load inference results"
      ],
      "metadata": {
        "id": "hdTgLIsFm_7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the output folders for each target sound analysed so far\n",
        "sound_folders = os.listdir(os.path.join(drive_output_directory, dataset_folder))\n",
        "if 'raw_embeddings' in sound_folders:\n",
        "    sound_folders.remove('raw_embeddings')\n",
        "\n",
        "# Store results in dict\n",
        "all_results_dict = {}\n",
        "for sound in sound_folders:\n",
        "  # Load results csv\n",
        "  results_csv_path = os.path.join(drive_output_directory,\n",
        "                                  dataset_folder,\n",
        "                                  sound + '/inference_'+ sound + '.csv')\n",
        "  results_df = pd.read_csv(results_csv_path)\n",
        "\n",
        "  # Strip white space\n",
        "  results_df.columns = results_df.columns.str.strip()\n",
        "  results_df = results_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "\n",
        "  # Find the files with the target sound and store their habitat type\n",
        "  target_detected = results_df[results_df['label'] == sound]\n",
        "  source_files = list(target_detected['filename'])\n",
        "  habitat_type = [item[20] for item in source_files] # our data has the habitat type denoted at the 20th character in the filename\n",
        "  habitat_counts = Counter(habitat_type)\n",
        "  habitat_counts_dict = dict(habitat_counts)\n",
        "  all_results_dict[sound] = habitat_counts_dict"
      ],
      "metadata": {
        "id": "bR4zrclGbLZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot results"
      ],
      "metadata": {
        "id": "syaJ8J5KnH6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of subplots based on the number of sounds\n",
        "num_sounds = len(all_results_dict)\n",
        "fig, axes = plt.subplots(1, num_sounds, figsize=(num_sounds * 5, 5))\n",
        "\n",
        "# Check if there's only one sound to ensure 'axes' is iterable\n",
        "if num_sounds == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# Extract and sort all unique habitat types from all sound types\n",
        "unique_habitats = sorted(set(habitat for counts in all_results_dict.values() for habitat in counts))\n",
        "\n",
        "# Generate a color map for each habitat type\n",
        "color_map = plt.cm.get_cmap('viridis_r', len(unique_habitats))\n",
        "colors = {habitat: color_map(i) for i, habitat in enumerate(unique_habitats)}\n",
        "\n",
        "# Define the width of each bar\n",
        "bar_width = 0.35\n",
        "\n",
        "# Iterate over each sound and its respective axis\n",
        "for ax, (sound, counts) in zip(axes, all_results_dict.items()):\n",
        "    # Create bars for each habitat type within this sound type\n",
        "    for j, habitat in enumerate(unique_habitats):\n",
        "        # Set position and height for this bar then add it\n",
        "        position = j * bar_width\n",
        "        height = counts.get(habitat, 0)\n",
        "        ax.bar(position, height, bar_width, color=colors[habitat])\n",
        "\n",
        "    # Set labels, title, and y-ticks for this subplot\n",
        "    ax.set_title(sound)\n",
        "    ax.set_xticks([j * bar_width for j in range(len(unique_habitats))])\n",
        "    ax.set_xticklabels(unique_habitats)\n",
        "    ax.set_xlabel('Habitat type')\n",
        "    ax.set_ylabel('Count')\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dClgBx78UFiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary findings\n",
        "Using a full run through of each dataset and the target sounds, we oberved some preliminary findings.\n",
        "\n",
        "**Indonesia**\n",
        "\n",
        "We observe a difference in the rate of occurence for our first two target sounds between healthy (H) and degraded (D) reefs. We can also see what looks like evidence for recovery of these vocalisations within the soundscape of restored (R) sites. Interestingly, for Parrotfish grazing, we observe the highest rate on restored reefs. Parrotfish grazing is an important ecological function, which could have implications for restoration.\n",
        "<img src=\"https://raw.githubusercontent.com/BenUCL/surfperch/surfperch/manuscript-additions/colab_pics/indo_results_fig.png\" width=\"100%\">\n",
        "\n",
        "**Great Barrier Reef**\n",
        "\n",
        "As expected we observe a higher rate of two vocalisations in  recordings from the higher fish diversity sites (H) than those where this was low (L). However, the charismatic whooping from the [Ambon Damselfish](https://fishbase.mnhn.fr/summary/5715) appears more abundant on the lower diversity reef sites which could be indicative of the habitat its selecting.\n",
        "<img src=\"https://raw.githubusercontent.com/BenUCL/surfperch/surfperch/manuscript-additions/colab_pics/aus_results_fig.png\" width=\"100%\">\n",
        "\n",
        "\n",
        "**Philippines**\n",
        "\n",
        "CIOC revealed two frequently occuring sounds from the  dataset: short purrs, often known as 'pulse trains' that typically originate from damselfish, and, pops. The classifier found both occured more often in recordings from the reefs in a marine protected area (P) than those in unprotected (P) areas.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/BenUCL/surfperch/surfperch/manuscript-additions/colab_pics/phil_results_fig.png\" width=\"66%\">\n",
        "\n",
        "\n",
        "**Conclusions**\n",
        "\n",
        "Importantly, these results are only preliminary. We only used 15-25 hrs from each dataset for the purposes of this demo. These preliminary results can be reproduced if you replace the 'SurfPerch Demo with Calling in Our Corals Outputs' folder in your Google Drive with the same folder from our published run through here: (TODO add link)\n",
        "\n",
        "Scientists are now working with the full Calling in Our Corals outputs and the Perch framework to study this data in depth. We would love your help! By annotating more data on the [CIOC platform](https://artsandculture.google.com/experiment/calling-in-our-corals/zgFx1tMqeIZyTw?hl=en) this will contribute to monitoring reef restoration globally.\n",
        "\n",
        "We hope this gives you an idea of how the Perch framework can be useful for real world bioacoustic challenges! We encourage users to adapt this to their own data, and use the results files to conduct data analysis on their own problem sets."
      ],
      "metadata": {
        "id": "YUnwj58yBhuH"
      }
    }
  ]
}