{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEoybFElAON3"
   },
   "source": [
    "# Agile Modeling for Bioacoustics.\n",
    "\n",
    "This notebook provides a single-machine workflow for using pre-trained models to embed raw audio files, search, and create classifiers for target signals. This notebook is ideal for a single machine with a GPU for accelarated embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gntw2Wq9Atpp"
   },
   "source": [
    "## Configuration and Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "58HLTIdcAzte"
   },
   "outputs": [],
   "source": [
    " #@title Imports. { vertical-output: true }\n",
    "\n",
    "import collections\n",
    "from etils import epath\n",
    "from ml_collections import config_dict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from chirp.inference import colab_utils\n",
    "colab_utils.initialize(use_tf_gpu=True, disable_warnings=True)\n",
    "\n",
    "from chirp import audio_utils\n",
    "from chirp.inference import embed_lib\n",
    "from chirp.inference import tf_examples\n",
    "from chirp.inference import models\n",
    "from chirp.models import metrics\n",
    "from chirp.inference.search import bootstrap\n",
    "from chirp.inference.search import search\n",
    "from chirp.inference.search import display\n",
    "from chirp.inference.classify import classify\n",
    "from chirp.inference.classify import data_lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "_fMOBVK9A_O1"
   },
   "outputs": [],
   "source": [
    "#@title Basic Configuration. { vertical-output: true }\n",
    "\n",
    "# Define the model: Usually perch or birdnet.\n",
    "model_choice = 'perch'  #@param\n",
    "# Set the base directory for the project.\n",
    "working_dir = '/tmp/agile'  #@param\n",
    "\n",
    "# Set the embedding and labeled data directories.\n",
    "embeddings_path = epath.Path(working_dir) / 'embeddings'\n",
    "labeled_data_path = epath.Path(working_dir) / 'labeled'\n",
    "embeddings_glob = embeddings_path / 'embeddings-*'\n",
    "\n",
    "# OPTIONAL: Set up separation model.\n",
    "separation_model_key = 'separator_model_tf'  #@param\n",
    "separation_model_path = ''  #@param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "2c4gJ6S2ETKu"
   },
   "outputs": [],
   "source": [
    "#@title Load Existing Project State and Models. { vertical-output: true }\n",
    "\n",
    "# If you have already computed embeddings, run this cell to load models\n",
    "# and find existing data.\n",
    "\n",
    "if (embeddings_path / 'config.json').exists():\n",
    "  # Get relevant info from the embedding configuration.\n",
    "  bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_config(\n",
    "      embeddings_path=embeddings_path,\n",
    "      annotated_path=labeled_data_path)\n",
    "  project_state = bootstrap.BootstrapState(bootstrap_config)\n",
    "\n",
    "  if (bootstrap_config.model_key == 'separate_embed_model'\n",
    "      and not separation_model_path.strip()):\n",
    "    separation_model_key = 'separator_model_tf'\n",
    "    separation_model_path = bootstrap_config.model_config.separator_model_tf_config.model_path\n",
    "\n",
    "# Load separation model.\n",
    "if separation_model_path:\n",
    "  separation_config = config_dict.ConfigDict({\n",
    "      'model_path': separation_model_path,\n",
    "      'frame_size': 32000,\n",
    "      'sample_rate': 32000,\n",
    "  })\n",
    "  separator = models.model_class_map()[\n",
    "      separation_model_key].from_config(separation_config)\n",
    "  print('Loaded separator model at {}'.format(separation_model_path))\n",
    "else:\n",
    "  print('No separation model loaded.')\n",
    "  separator = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XpWruWMArWo"
   },
   "source": [
    "## Embed Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "qx-SWjFYALok"
   },
   "outputs": [],
   "source": [
    "#@title Embedding Configuration. { vertical-output: true }\n",
    "\n",
    "config = config_dict.ConfigDict()\n",
    "config.embed_fn_config = config_dict.ConfigDict()\n",
    "config.embed_fn_config.model_config = config_dict.ConfigDict()\n",
    "\n",
    "# IMPORTANT: Select the targe audio files.\n",
    "# source_file_patterns should contain a list of globs of audio files, like:\n",
    "# ['/home/me/*.wav', '/home/me/other/*.flac']\n",
    "config.source_file_patterns = ['']  #@param\n",
    "config.output_dir = embeddings_path.as_posix()\n",
    "\n",
    "# For Perch, set the perch_tfhub_model_version, and the model will load\n",
    "# automagically from TFHub. Alternatively, set the model path for a local\n",
    "# copy of the model.\n",
    "# Note that only one of perch_model_path and perch_tfhub_version should be set.\n",
    "perch_tfhub_version = 4  #@param\n",
    "perch_model_path = ''  #@param\n",
    "\n",
    "# For BirdNET, point to the specific tflite file.\n",
    "birdnet_model_path = ''  #@param\n",
    "if model_choice == 'perch':\n",
    "  config.embed_fn_config.model_key = 'taxonomy_model_tf'\n",
    "  config.embed_fn_config.model_config.window_size_s = 5.0\n",
    "  config.embed_fn_config.model_config.hop_size_s = 5.0\n",
    "  config.embed_fn_config.model_config.sample_rate = 32000\n",
    "  config.embed_fn_config.model_config.tfhub_version = perch_tfhub_version\n",
    "  config.embed_fn_config.model_config.model_path = perch_model_path\n",
    "elif model_choice == 'birdnet':\n",
    "  config.embed_fn_config.model_key = 'birdnet'\n",
    "  config.embed_fn_config.model_config.window_size_s = 3.0\n",
    "  config.embed_fn_config.model_config.hop_size_s = 3.0\n",
    "  config.embed_fn_config.model_config.sample_rate = 48000\n",
    "  config.embed_fn_config.model_config.model_path = birdnet_model_path\n",
    "  # Note: The v2_1 class list is appropriate for Birdnet 2.1, 2.2, and 2.3.\n",
    "  config.embed_fn_config.model_config.class_list_name = 'birdnet_v2_1'\n",
    "  config.embed_fn_config.model_config.num_tflite_threads = 4\n",
    "\n",
    "# Only write embeddings to reduce size.\n",
    "config.embed_fn_config.write_embeddings = True\n",
    "config.embed_fn_config.write_logits = False\n",
    "config.embed_fn_config.write_separated_audio = False\n",
    "config.embed_fn_config.write_raw_audio = False\n",
    "\n",
    "# Number of parent directories to include in the filename.\n",
    "config.embed_fn_config.file_id_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "jb-pEadVDidv"
   },
   "outputs": [],
   "source": [
    "#@title Set up. { vertical-output: true }\n",
    "\n",
    "# Set up the embedding function, including loading models.\n",
    "embed_fn = embed_lib.EmbedFn(**config.embed_fn_config)\n",
    "print('\\n\\nLoading model(s)...')\n",
    "embed_fn.setup()\n",
    "\n",
    "# Create output directory and write the configuration.\n",
    "output_dir = epath.Path(config.output_dir)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "embed_lib.maybe_write_config(config, output_dir)\n",
    "\n",
    "# Create SourceInfos.\n",
    "source_infos = embed_lib.create_source_infos(\n",
    "    config.source_file_patterns,\n",
    "    num_shards_per_file=config.get('num_shards_per_file', -1),\n",
    "    shard_len_s=config.get('shard_len_s', -1))\n",
    "print(f'Found {len(source_infos)} source infos.')\n",
    "\n",
    "print('\\n\\nTest-run of model...')\n",
    "window_size_s = config.embed_fn_config.model_config.window_size_s\n",
    "sr = config.embed_fn_config.model_config.sample_rate\n",
    "z = np.zeros([int(sr * window_size_s)])\n",
    "embed_fn.embedding_model.embed(z)\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Dvnwf_LZDkBf"
   },
   "outputs": [],
   "source": [
    "#@title Run embedding. { vertical-output: true }\n",
    "\n",
    "# Uses multiple threads to load audio before embedding.\n",
    "# This tends to be faster, but can fail if any audio files are corrupt.\n",
    "\n",
    "embed_fn.min_audio_s = 1.0\n",
    "record_file = (output_dir / 'embeddings.tfrecord').as_posix()\n",
    "succ, fail = 0, 0\n",
    "\n",
    "existing_embedding_ids = embed_lib.get_existing_source_ids(\n",
    "    output_dir, 'embeddings-*')\n",
    "\n",
    "new_source_infos = embed_lib.get_new_source_infos(\n",
    "    source_infos, existing_embedding_ids, config.embed_fn_config.file_id_depth)\n",
    "\n",
    "print(f'Found {len(new_source_infos)} existing embedding ids.'\n",
    "      f'Processing {len(new_source_infos)} new source infos. ')\n",
    "\n",
    "audio_iterator = audio_utils.multi_load_audio_window(\n",
    "    filepaths=[s.filepath for s in new_source_infos],\n",
    "    offsets=[s.shard_num * s.shard_len_s for s in new_source_infos],\n",
    "    sample_rate=config.embed_fn_config.model_config.sample_rate,\n",
    "    window_size_s=config.get('shard_len_s', -1.0),\n",
    ")\n",
    "with tf_examples.EmbeddingsTFRecordMultiWriter(\n",
    "    output_dir=output_dir, num_files=config.get('tf_record_shards', 1)) as file_writer:\n",
    "  for source_info, audio in tqdm.tqdm(\n",
    "      zip(new_source_infos, audio_iterator), total=len(new_source_infos)):\n",
    "    file_id = source_info.file_id(config.embed_fn_config.file_id_depth)\n",
    "    offset_s = source_info.shard_num * source_info.shard_len_s\n",
    "    example = embed_fn.audio_to_example(file_id, offset_s, audio)\n",
    "    if example is None:\n",
    "      fail += 1\n",
    "      continue\n",
    "    file_writer.write(example.SerializeToString())\n",
    "    succ += 1\n",
    "  file_writer.flush()\n",
    "print(f'\\n\\nSuccessfully processed {succ} source_infos, failed {fail} times.')\n",
    "\n",
    "fns = [fn for fn in output_dir.glob('embeddings-*')]\n",
    "ds = tf.data.TFRecordDataset(fns)\n",
    "parser = tf_examples.get_example_parser()\n",
    "ds = ds.map(parser)\n",
    "for ex in ds.as_numpy_iterator():\n",
    "  print(ex['filename'])\n",
    "  print(ex['embedding'].shape, flush=True)\n",
    "  break\n",
    "\n",
    "# Load/refresh bootstrap_config for subsequent steps.\n",
    "print('\\nRefreshing bootstrap_config.', flush=True)\n",
    "bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_config(\n",
    "    embeddings_path=embeddings_path,\n",
    "    annotated_path=labeled_data_path)\n",
    "\n",
    "project_state = bootstrap.BootstrapState(bootstrap_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0p0qkxcFSG0"
   },
   "source": [
    "## Search Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eML2kUhuGfZQ"
   },
   "source": [
    "### Query Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "xc1ExwK2GbQx"
   },
   "outputs": [],
   "source": [
    "#@title Load query audio. { vertical-output: true }\n",
    "\n",
    "# Point to an audio file, Xeno-Canto id (like 'xc12345') or audio file URL.\n",
    "audio_path = 'xc871667'  #@param\n",
    "# Muck around with manual selection of the query start time...\n",
    "start_s = 0  #@param\n",
    "\n",
    "window_s = bootstrap_config.model_config['window_size_s']\n",
    "sample_rate = bootstrap_config.model_config['sample_rate']\n",
    "audio = audio_utils.load_audio(audio_path, sample_rate)\n",
    "\n",
    "# Display the full file.\n",
    "display.plot_audio_melspec(audio, sample_rate)\n",
    "\n",
    "# Display the selected window.\n",
    "print('-' * 80)\n",
    "print('Selected audio window:')\n",
    "st = int(start_s * sample_rate)\n",
    "end = int(st + window_s * sample_rate)\n",
    "if end > audio.shape[0]:\n",
    "  end = audio.shape[0]\n",
    "  st = max([0, int(end - window_s * sample_rate)])\n",
    "audio_window = audio[st:end]\n",
    "display.plot_audio_melspec(audio_window, sample_rate)\n",
    "\n",
    "query_audio = audio_window\n",
    "sep_outputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uELuAcwjGj4h"
   },
   "outputs": [],
   "source": [
    "#@title Separate the target audio window { vertical-output: true }\n",
    "\n",
    "if separator is not None:\n",
    "  sep_outputs = separator.embed(audio_window)\n",
    "\n",
    "  for c in range(sep_outputs.separated_audio.shape[0]):\n",
    "    print(f'Channel {c}')\n",
    "    display.plot_audio_melspec(sep_outputs.separated_audio[c, :], sample_rate)\n",
    "    print('-' * 80)\n",
    "else:\n",
    "  sep_outputs = None\n",
    "  print('No separation model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "atpdag0FGkud"
   },
   "outputs": [],
   "source": [
    "#@title Select the query channel. { vertical-output: true }\n",
    "\n",
    "query_label = 'some_audio'  #@param\n",
    "query_channel = 0  #@param\n",
    "\n",
    "if query_channel < 0 or sep_outputs is None:\n",
    "  query_audio = audio_window\n",
    "else:\n",
    "  query_audio = sep_outputs.separated_audio[query_channel].copy()\n",
    "\n",
    "display.plot_audio_melspec(query_audio, sample_rate)\n",
    "\n",
    "outputs = project_state.embedding_model.embed(query_audio)\n",
    "query = outputs.pooled_embeddings('first', 'first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_Sx9wlJGo9y"
   },
   "source": [
    "### Execute Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tOV_G29mGm_Z"
   },
   "outputs": [],
   "source": [
    "#@title Run Top-K Search. { vertical-output: true }\n",
    "\n",
    "# Number of search results to capture.\n",
    "top_k = 25  #@param\n",
    "\n",
    "# Target distance for search results.\n",
    "# This lets us try to hone in on a 'classifier boundary' instead of just\n",
    "# looking at the closest matches.\n",
    "# Set to 'None' for raw 'best results' search.\n",
    "target_score = None  #@param\n",
    "\n",
    "metric = 'mip'  #@param['euclidean', 'mip', 'cosine']\n",
    "\n",
    "random_sample = False  #@param\n",
    "\n",
    "ds = project_state.create_embeddings_dataset()\n",
    "results, all_scores = search.search_embeddings_parallel(\n",
    "    ds, query,\n",
    "    hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
    "    top_k=top_k, target_score=target_score, score_fn=metric,\n",
    "    random_sample=random_sample)\n",
    "\n",
    "# Plot histogram of distances\n",
    "ys, _, _ = plt.hist(all_scores, bins=128, density=True)\n",
    "hit_scores = [r.score for r in results.search_results]\n",
    "plt.scatter(hit_scores, np.zeros_like(hit_scores), marker='|',\n",
    "            color='r', alpha=0.5)\n",
    "\n",
    "plt.xlabel(metric)\n",
    "plt.ylabel('density')\n",
    "if target_score is not None:\n",
    "  plt.plot([target_score, target_score], [0.0, np.max(ys)], 'r:')\n",
    "  # Compute the proportion of scores < target_score\n",
    "  hit_percentage = (all_scores < target_score).mean()\n",
    "  print(f'score < target_score percentage : {hit_percentage:5.3f}')\n",
    "min_score = np.min(all_scores)\n",
    "plt.plot([min_score, min_score], [0.0, np.max(ys)], 'g:')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "a8SYu-M2GsZx"
   },
   "outputs": [],
   "source": [
    "#@title Display results. { vertical-output: true }\n",
    "\n",
    "display.display_search_results(\n",
    "    results, sample_rate, project_state.source_map,\n",
    "    checkbox_labels=[query_label, 'unknown'],\n",
    "    max_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-wi-TOGDGvh5"
   },
   "outputs": [],
   "source": [
    "#@title Write annotated examples. { vertical-output: true }\n",
    "\n",
    "results.write_labeled_data(bootstrap_config.annotated_path,\n",
    "                           project_state.embedding_model.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbZkOXncFYKm"
   },
   "source": [
    "## Active Learning for a Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "NcLdypLeHmss"
   },
   "outputs": [],
   "source": [
    "# @title Load+Embed the Labeled Dataset. { vertical-output: true }\n",
    "\n",
    "# Time-pooling strategy for examples longer than the model's window size.\n",
    "time_pooling = 'mean'  # @param\n",
    "\n",
    "merged = data_lib.MergedDataset.from_folder_of_folders(\n",
    "    base_dir=labeled_data_path,\n",
    "    embedding_model=project_state.embedding_model,\n",
    "    time_pooling=time_pooling,\n",
    "    load_audio=False,\n",
    "    target_sample_rate=-2,\n",
    "    audio_file_pattern='*',\n",
    "    embedding_config_hash=bootstrap_config.embedding_config_hash(),\n",
    ")\n",
    "\n",
    "# Label distribution\n",
    "lbl_counts = np.sum(merged.data['label_hot'], axis=0)\n",
    "print('num classes :', (lbl_counts > 0).sum())\n",
    "print('mean ex / class :', lbl_counts.sum() / (lbl_counts > 0).sum())\n",
    "print('min ex / class :', (lbl_counts + (lbl_counts == 0) * 1e6).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "7mY5rIiDHoE0"
   },
   "outputs": [],
   "source": [
    "#@title Train small model over embeddings. { vertical-output: true }\n",
    "\n",
    "# Number of random training examples to choose form each class.\n",
    "# Set exactly one of train_ratio and train_examples_per_class\n",
    "train_ratio = None  #@param\n",
    "train_examples_per_class = 2  #@param\n",
    "\n",
    "# Number of random re-trainings. Allows judging model stability.\n",
    "num_seeds = 1  #@param\n",
    "\n",
    "# Classifier training hyperparams.\n",
    "# These should be good defaults.\n",
    "batch_size = 32\n",
    "num_epochs = 128\n",
    "num_hiddens = -1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "metrics = collections.defaultdict(list)\n",
    "for seed in tqdm.tqdm(range(num_seeds)):\n",
    "  if num_hiddens > 0:\n",
    "    model = classify.get_two_layer_model(\n",
    "        num_hiddens, merged.embedding_dim, merged.num_classes)\n",
    "  else:\n",
    "    model = classify.get_linear_model(\n",
    "        merged.embedding_dim, merged.num_classes)\n",
    "  run_metrics = classify.train_embedding_model(\n",
    "      model, merged, train_ratio, train_examples_per_class,\n",
    "      num_epochs, seed, batch_size, learning_rate)\n",
    "  metrics['acc'].append(run_metrics.top1_accuracy)\n",
    "  metrics['auc_roc'].append(run_metrics.auc_roc)\n",
    "  metrics['cmap'].append(run_metrics.cmap_value)\n",
    "  metrics['maps'].append(run_metrics.class_maps)\n",
    "  metrics['test_logits'].append(run_metrics.test_logits)\n",
    "\n",
    "mean_acc = np.mean(metrics['acc'])\n",
    "mean_auc = np.mean(metrics['auc_roc'])\n",
    "mean_cmap = np.mean(metrics['cmap'])\n",
    "# Merge the test_logits into a single array.\n",
    "test_logits = {\n",
    "    k: np.concatenate([logits[k] for logits in metrics['test_logits']])\n",
    "    for k in metrics['test_logits'][0].keys()\n",
    "}\n",
    "\n",
    "print(f'acc:{mean_acc:5.2f}, auc_roc:{mean_auc:5.2f}, cmap:{mean_cmap:5.2f}')\n",
    "for lbl, auc in zip(merged.labels, run_metrics.class_maps):\n",
    "  if np.isnan(auc):\n",
    "    continue\n",
    "  print(f'\\n{lbl:8s}, auc_roc:{auc:5.2f}')\n",
    "  colab_utils.prstats(f'test_logits({lbl})',\n",
    "                      test_logits[merged.labels.index(lbl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-0Kg43YhH0p3"
   },
   "outputs": [],
   "source": [
    "#@title Run model on target unlabeled data. { vertical-output: true }\n",
    "\n",
    "# Choose the target class to work with.\n",
    "target_class = 'some_audio'  #@param\n",
    "# Choose a target logit; will display results close to the target.\n",
    "# Set to None to get the highest-logit examples.\n",
    "target_logit = None  #@param\n",
    "# Number of results to display.\n",
    "num_results = 25  #@param\n",
    "\n",
    "# Create the embeddings dataset.\n",
    "embeddings_ds = tf_examples.create_embeddings_dataset(\n",
    "    embeddings_path, file_glob='embeddings-*')\n",
    "target_class_idx = merged.labels.index(target_class)\n",
    "results, all_logits = search.classifer_search_embeddings_parallel(\n",
    "    embeddings_classifier=model,\n",
    "    target_index=target_class_idx,\n",
    "    embeddings_dataset=embeddings_ds,\n",
    "    hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
    "    target_score=target_logit,\n",
    "    top_k=num_results\n",
    ")\n",
    "\n",
    "# Plot the histogram of logits.\n",
    "_, ys, _ = plt.hist(all_logits, bins=128, density=True)\n",
    "plt.xlabel(f'{target_class} logit')\n",
    "plt.ylabel('density')\n",
    "# plt.yscale('log')\n",
    "plt.plot([target_logit, target_logit], [0.0, np.max(ys)], 'r:')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "gHI2WJwPH2Wy"
   },
   "outputs": [],
   "source": [
    "#@title Display results for the target label. { vertical-output: true }\n",
    "\n",
    "display_labels = merged.labels\n",
    "\n",
    "extra_labels = []  #@param\n",
    "for label in extra_labels:\n",
    "  if label not in merged.labels:\n",
    "    display_labels += (label,)\n",
    "if 'unknown' not in merged.labels:\n",
    "  display_labels += ('unknown',)\n",
    "\n",
    "display.display_search_results(\n",
    "    results, project_state.embedding_model.sample_rate,\n",
    "    project_state.source_map,\n",
    "    checkbox_labels=display_labels,\n",
    "    max_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "mDCcKfBGH4b_"
   },
   "outputs": [],
   "source": [
    "#@title Add selected results to the labeled data. { vertical-output: true }\n",
    "\n",
    "results.write_labeled_data(\n",
    "    bootstrap_config.annotated_path,\n",
    "    project_state.embedding_model.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNw_uivxIJda"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XdpUiaYiIMIp"
   },
   "outputs": [],
   "source": [
    "#@title Write classifier inference CSV. { vertical-output: true }\n",
    "\n",
    "threshold = 1.0  #@param\n",
    "output_filepath = '/tmp/inference.csv'  #@param\n",
    "\n",
    "# Create the embeddings dataset.\n",
    "embeddings_ds = tf_examples.create_embeddings_dataset(\n",
    "    embeddings_path, file_glob='embeddings-*')\n",
    "\n",
    "def classify_batch(batch):\n",
    "  \"\"\"Classify a batch of embeddings.\"\"\"\n",
    "  emb = batch[tf_examples.EMBEDDING]\n",
    "  emb_shape = tf.shape(emb)\n",
    "  flat_emb = tf.reshape(emb, [-1, emb_shape[-1]])\n",
    "  logits = model(flat_emb)\n",
    "  logits = tf.reshape(\n",
    "      logits, [emb_shape[0], emb_shape[1], tf.shape(logits)[-1]])\n",
    "  # Take the maximum logit over channels.\n",
    "  logits = tf.reduce_max(logits, axis=-2)\n",
    "  batch['logits'] = logits\n",
    "  return batch\n",
    "\n",
    "inference_ds = tf_examples.create_embeddings_dataset(\n",
    "    embeddings_path, file_glob='embeddings-*')\n",
    "inference_ds = inference_ds.map(\n",
    "    classify_batch, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "with open(output_filepath, 'w') as f:\n",
    "  # Write column headers.\n",
    "  headers = ['filename', 'timestamp_s', 'label', 'logit']\n",
    "  f.write(', '.join(headers) + '\\n')\n",
    "  for ex in tqdm.tqdm(inference_ds.as_numpy_iterator()):\n",
    "    for t in range(ex['logits'].shape[0]):\n",
    "      for i, label in enumerate(merged.labels):\n",
    "        if ex['logits'][t, i] > threshold:\n",
    "          offset = ex['timestamp_s'] + t * bootstrap_config.embedding_hop_size_s\n",
    "          logit = '{:.2f}'.format(ex['logits'][t, i])\n",
    "          row = [ex['filename'].decode('utf-8'),\n",
    "                 '{:.2f}'.format(offset),\n",
    "                 label, logit]\n",
    "          f.write(', '.join(row) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSqxSk74EIgs"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
