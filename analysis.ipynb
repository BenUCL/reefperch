{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndV0dmyzhpHE"
   },
   "source": [
    "# Analysis of Bioacoustic Data\n",
    "\n",
    "This notebook provides tools for analyzing data using a custom classifier (developed with `agile_modeling.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "i984ftjPcxDu"
   },
   "outputs": [],
   "source": [
    "#@title Imports. { vertical-output: true }\n",
    "\n",
    "import collections\n",
    "from etils import epath\n",
    "from ml_collections import config_dict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chirp.inference import colab_utils\n",
    "colab_utils.initialize(use_tf_gpu=True, disable_warnings=True)\n",
    "\n",
    "from chirp.inference import interface\n",
    "from chirp.inference import tf_examples\n",
    "from chirp.inference.search import bootstrap\n",
    "from chirp.inference.search import search\n",
    "from chirp.inference.search import display\n",
    "from chirp.inference.classify import classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TRETHuu1h7uZ"
   },
   "outputs": [],
   "source": [
    "#@title Basic Configuration. { vertical-output: true }\n",
    "\n",
    "# Define the model: Usually perch or birdnet.\n",
    "model_choice = 'perch'  #@param\n",
    "# Set the base directory for the project.\n",
    "working_dir = '/tmp/agile'  #@param\n",
    "\n",
    "# Set the embedding and labeled data directories.\n",
    "embeddings_path = epath.Path(working_dir) / 'embeddings'\n",
    "labeled_data_path = epath.Path(working_dir) / 'labeled'\n",
    "custom_classifier_path = epath.Path(working_dir) / 'custom_classifier'\n",
    "embeddings_glob = embeddings_path / 'embeddings-*'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ake6Xk_Hh-nN"
   },
   "outputs": [],
   "source": [
    "#@title Load Existing Project State and Models. { vertical-output: true }\n",
    "\n",
    "# If you have already computed embeddings, run this cell to load models\n",
    "# and find existing data.\n",
    "\n",
    "if (embeddings_path / 'config.json').exists():\n",
    "  # Get relevant info from the embedding configuration.\n",
    "  bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_config(\n",
    "      embeddings_path=embeddings_path,\n",
    "      annotated_path=labeled_data_path)\n",
    "  project_state = bootstrap.BootstrapState(bootstrap_config)\n",
    "\n",
    "cfg = config_dict.ConfigDict({\n",
    "    'model_path': custom_classifier_path,\n",
    "    'logits_key': 'custom',\n",
    "})\n",
    "loaded_model = interface.LogitsOutputHead.from_config(cfg)\n",
    "model = loaded_model.logits_model\n",
    "class_list = loaded_model.class_list\n",
    "print('Loaded custom model with classes: ')\n",
    "print('\\t' + '\\n\\t'.join(class_list.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Ur03VoLyuBHR"
   },
   "outputs": [],
   "source": [
    "#@title Write classifier inference CSV. { vertical-output: true }\n",
    "\n",
    "output_filepath = '/tmp/inference.csv'  #@param\n",
    "\n",
    "# Set detection thresholds.\n",
    "default_threshold = 0.0  #@param\n",
    "if default_threshold is None:\n",
    "  # In this case, all logits are written. This can lead to very large CSV files.\n",
    "  class_thresholds = None\n",
    "else:\n",
    "  class_thresholds = collections.defaultdict(lambda: default_threshold)\n",
    "  # Set per-class thresholds here.\n",
    "  class_thresholds['my_class'] = 1.0\n",
    "\n",
    "# Classes for which we do not want to write detections.\n",
    "exclude_classes = ['unknown']  #@param\n",
    "\n",
    "# include_classes is ignored if empty.\n",
    "# If non-empty, only scores for these classes will be written.\n",
    "include_classes = []  #@param\n",
    "\n",
    "# Create the embeddings dataset.\n",
    "embeddings_ds = tf_examples.create_embeddings_dataset(\n",
    "    embeddings_path, file_glob='embeddings-*')\n",
    "\n",
    "classify.write_inference_csv(\n",
    "    embeddings_ds=embeddings_ds,\n",
    "    model=model,\n",
    "    labels=class_list.classes,\n",
    "    output_filepath=output_filepath,\n",
    "    threshold=class_thresholds,\n",
    "    embedding_hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
    "    include_classes=include_classes,\n",
    "    exclude_classes=exclude_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdJmpn0XzMj6"
   },
   "source": [
    "## Call Density Estimation\n",
    "\n",
    "See 'All Thresholds Barred': https://arxiv.org/abs/2402.15360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-lhqypjsu2L9"
   },
   "outputs": [],
   "source": [
    "#@title Validation and Call Density. { vertical-output: true }\n",
    "# For validation, we select random samples from logarithmic-quantile bins.\n",
    "\n",
    "target_class = 'my_class'  #@param\n",
    "\n",
    "num_bins = 4  #@param\n",
    "samples_per_bin = 50  #@param\n",
    "# The highest bin contains 2**-num_bins of the data.\n",
    "top_k = samples_per_bin * 2**(num_bins + 1)\n",
    "\n",
    "embeddings_ds = tf_examples.create_embeddings_dataset(\n",
    "    embeddings_path, file_glob='embeddings-*')\n",
    "results, all_logits = search.classifer_search_embeddings_parallel(\n",
    "    embeddings_classifier=model,\n",
    "    target_index=class_list.classes.index(target_class),\n",
    "    random_sample=True,\n",
    "    top_k=top_k,\n",
    "    hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
    "    embeddings_dataset=embeddings_ds,\n",
    ")\n",
    "\n",
    "# Pick samples_per_bin examples from each quantile.\n",
    "def get_quantile_bounds(n_bins):\n",
    "  lowers = [1.0 - 1.0 / 2**(k + 1) for k in range(n_bins - 1)]\n",
    "  return np.array([0.0] + lowers + [1.0])\n",
    "\n",
    "bounds = get_quantile_bounds(num_bins)\n",
    "q_bounds = np.quantile(all_logits, bounds)\n",
    "binned = [[] for _ in range(num_bins)]\n",
    "for r in results.search_results:\n",
    "  bin = np.argmax(r.score < q_bounds) - 1\n",
    "  binned[bin].append(r)\n",
    "binned = [np.random.choice(b, samples_per_bin) for b in binned]\n",
    "\n",
    "combined = []\n",
    "for b in binned:\n",
    "  combined.extend(b)\n",
    "np.random.shuffle(combined)\n",
    "\n",
    "samples_per_page = 10\n",
    "page_state = display.PageState(np.ceil(len(combined) / samples_per_page))\n",
    "\n",
    "display.display_paged_results(\n",
    "    search.TopKSearchResults(combined, len(combined)),\n",
    "    page_state, samples_per_page,\n",
    "    embedding_sample_rate=project_state.embedding_model.sample_rate,\n",
    "    source_map=project_state.source_map,\n",
    "    exclusive_labels=True,\n",
    "    checkbox_labels=[target_class, f'not {target_class}', 'unsure'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "A30usazfu8h8"
   },
   "outputs": [],
   "source": [
    "#@title Collate results and write validation log. { vertical-output: true }\n",
    "\n",
    "validation_log_filepath = epath.Path(working_dir) / 'validation.csv'\n",
    "\n",
    "filenames = []\n",
    "timestamp_offsets = []\n",
    "scores = []\n",
    "is_pos = []\n",
    "\n",
    "for r in combined:\n",
    "  if not r.label_widgets: continue\n",
    "  value = r.label_widgets[0].value\n",
    "  if value is None:\n",
    "    continue\n",
    "  filenames.append(r.filename)\n",
    "  scores.append(r.score)\n",
    "  timestamp_offsets.append(r.timestamp_offset)\n",
    "  if value == target_class:\n",
    "    is_pos.append(1)\n",
    "  elif value == f'not {target_class}':\n",
    "    is_pos.append(-1)\n",
    "  elif value == 'unsure':\n",
    "    is_pos.append(0)\n",
    "\n",
    "label = [target_class for _ in range(len(filenames))]\n",
    "log = pd.DataFrame({\n",
    "    'filenames': filenames,\n",
    "    'timestamp_offsets': timestamp_offsets,\n",
    "    'scores': scores,\n",
    "    'is_pos': is_pos})\n",
    "log.to_csv(output_filepath, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uZHVzPttwGZ2"
   },
   "outputs": [],
   "source": [
    "#@title Estimate Model Quality and Call Density. { vertical-output: true }\n",
    "\n",
    "import scipy\n",
    "\n",
    "# Collect validated labels by bin.\n",
    "bin_pos = [0 for i in range(num_bins)]\n",
    "bin_neg = [0 for i in range(num_bins)]\n",
    "for score, pos in zip(scores, is_pos):\n",
    "  bin = np.argmax(score < q_bounds) - 1\n",
    "  if pos == 1:\n",
    "    bin_pos[bin] += 1\n",
    "  elif pos == -1:\n",
    "    bin_neg[bin] += 1\n",
    "\n",
    "# Create beta distributions.\n",
    "prior = 0.1\n",
    "betas = [scipy.stats.beta(p + prior, n + prior)\n",
    "         for p, n in zip(bin_pos, bin_neg)]\n",
    "# MLE positive rate in each bin.\n",
    "mle_b = np.array([bin_pos[b] / (bin_pos[b] + bin_neg[b] + 1e-6)\n",
    "                  for b in range(num_bins)])\n",
    "# Probability of each bin, P(b).\n",
    "p_b = np.array([2**-k for k in range(1, num_bins)] + [2**(-num_bins + 1)])\n",
    "\n",
    "# MLE total call density.\n",
    "q_mle = np.dot(mle_b, p_b)\n",
    "\n",
    "num_beta_samples = 10_000\n",
    "q_betas = []\n",
    "for _ in range(num_beta_samples):\n",
    "  qs_pos = np.array([b.rvs(size=1)[0] for b in betas])  # P(+|b)\n",
    "  q_beta = np.dot(qs_pos, p_b)\n",
    "  q_betas.append(q_beta)\n",
    "\n",
    "# Plot call density estimate.\n",
    "plt.figure(figsize=(10, 5))\n",
    "xs, ys, _ = plt.hist(q_betas, density=True, bins=25, alpha=0.25)\n",
    "plt.plot([q_mle, q_mle], [0.0, np.max(xs)], 'k:', alpha=0.75,\n",
    "         label='q_mle')\n",
    "\n",
    "low, high = np.quantile(q_betas, [0.05, 0.95])\n",
    "plt.plot([low, low], [0.0, np.max(xs)], 'g', alpha=0.75, label='low conf')\n",
    "plt.plot([high, high], [0.0, np.max(xs)], 'g', alpha=0.75, label='high conf')\n",
    "\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.xlabel('Call Rate (q)')\n",
    "plt.ylabel('P(q)')\n",
    "plt.title(f'Call Density Estimation ({target_class})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'MLE Call Density: {q_mle:.4f}')\n",
    "print(f'(Low/MLE/High) Call Density Estimate: ({low:5.4f} / {q_mle:5.4f} / {high:5.4f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "6PPrCBc-15k_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
