{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndV0dmyzhpHE"
   },
   "source": [
    "# Analysis of Bioacoustic Data\n",
    "\n",
    "This notebook provides tools for analyzing data using a custom classifier (developed with `agile_modeling.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "i984ftjPcxDu"
   },
   "outputs": [],
   "source": [
    "#@title Imports. { vertical-output: true }\n",
    "\n",
    "import collections\n",
    "from etils import epath\n",
    "from ml_collections import config_dict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from chirp.inference import colab_utils\n",
    "colab_utils.initialize(use_tf_gpu=True, disable_warnings=True)\n",
    "\n",
    "from chirp.inference import a2o_utils\n",
    "from chirp.inference import interface\n",
    "from chirp.inference import tf_examples\n",
    "from chirp.inference.search import bootstrap\n",
    "from chirp.inference.search import search\n",
    "from chirp.inference.search import display\n",
    "from chirp.inference.classify import classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "TRETHuu1h7uZ"
   },
   "outputs": [],
   "source": [
    "#@title Basic Configuration. { vertical-output: true }\n",
    "\n",
    "data_source = 'filesystem' #@param['filesystem', 'a2o']\n",
    "a2o_auth_token = '' #@param\n",
    "\n",
    "# Define the model: Usually perch or birdnet.\n",
    "model_choice = 'perch'  #@param\n",
    "# Set the base directory for the project.\n",
    "working_dir = '/tmp/agile'  #@param\n",
    "\n",
    "# Set the embedding and labeled data directories.\n",
    "labeled_data_path = epath.Path(working_dir) / 'labeled'\n",
    "custom_classifier_path = epath.Path(working_dir) / 'custom_classifier'\n",
    "\n",
    "# The embeddings_path should be detected automatically, but can be overridden.\n",
    "embeddings_path = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ake6Xk_Hh-nN"
   },
   "outputs": [],
   "source": [
    "#@title Load Existing Project State and Models. { vertical-output: true }\n",
    "\n",
    "# If you have already computed embeddings, run this cell to load models\n",
    "# and find existing data.\n",
    "\n",
    "if data_source == 'a2o':\n",
    "  embedding_config = a2o_utils.get_a2o_embeddings_config()\n",
    "  bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_config(\n",
    "      embedding_config=embedding_config,\n",
    "      annotated_path=labeled_data_path,\n",
    "      embeddings_glob = '*/embeddings-*')\n",
    "  embeddings_path = embedding_config.output_dir\n",
    "elif (embeddings_path\n",
    "      or (epath.Path(working_dir) / 'embeddings/config.json').exists()):\n",
    "  if not embeddings_path:\n",
    "    # Use the default embeddings path, as it seems we found a config there.\n",
    "    embeddings_path = epath.Path(working_dir) / 'embeddings'\n",
    "  # Get relevant info from the embedding configuration.\n",
    "  bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_path(\n",
    "      embeddings_path=embeddings_path,\n",
    "      annotated_path=labeled_data_path)\n",
    "else:\n",
    "  raise ValueError('No embedding configuration found.')\n",
    "\n",
    "project_state = bootstrap.BootstrapState(\n",
    "    bootstrap_config, a2o_auth_token=a2o_auth_token)\n",
    "\n",
    "cfg = config_dict.ConfigDict({\n",
    "    'model_path': custom_classifier_path,\n",
    "    'logits_key': 'custom',\n",
    "})\n",
    "loaded_model = interface.LogitsOutputHead.from_config(cfg)\n",
    "model = loaded_model.logits_model\n",
    "class_list = loaded_model.class_list\n",
    "print('Loaded custom model with classes: ')\n",
    "print('\\t' + '\\n\\t'.join(class_list.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Ur03VoLyuBHR"
   },
   "outputs": [],
   "source": [
    "#@title Write classifier inference CSV. { vertical-output: true }\n",
    "\n",
    "output_filepath = epath.Path(working_dir) / 'inference.csv'  #@param\n",
    "\n",
    "# Set detection thresholds.\n",
    "default_threshold = 0.0  #@param\n",
    "if default_threshold is None:\n",
    "  # In this case, all logits are written. This can lead to very large CSV files.\n",
    "  class_thresholds = None\n",
    "else:\n",
    "  class_thresholds = collections.defaultdict(lambda: default_threshold)\n",
    "  # Set per-class thresholds here.\n",
    "  class_thresholds['my_class'] = 1.0\n",
    "\n",
    "# Classes for which we do not want to write detections.\n",
    "exclude_classes = ['unknown']  #@param\n",
    "\n",
    "# include_classes is ignored if empty.\n",
    "# If non-empty, only scores for these classes will be written.\n",
    "include_classes = []  #@param\n",
    "\n",
    "# Create the embeddings dataset.\n",
    "embeddings_ds = tf_examples.create_embeddings_dataset(\n",
    "    embeddings_path, file_glob='embeddings-*')\n",
    "\n",
    "classify.write_inference_csv(\n",
    "    embeddings_ds=embeddings_ds,\n",
    "    model=model,\n",
    "    labels=class_list.classes,\n",
    "    output_filepath=output_filepath,\n",
    "    threshold=class_thresholds,\n",
    "    embedding_hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
    "    include_classes=include_classes,\n",
    "    exclude_classes=exclude_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdJmpn0XzMj6"
   },
   "source": [
    "## Call Density Estimation\n",
    "\n",
    "See 'All Thresholds Barred': https://arxiv.org/abs/2402.15360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-lhqypjsu2L9"
   },
   "outputs": [],
   "source": [
    "#@title Validation and Call Density. { vertical-output: true }\n",
    "# For validation, we select random samples from logarithmic-quantile bins.\n",
    "\n",
    "target_class = 'my_class'  #@param\n",
    "\n",
    "num_bins = 5  #@param\n",
    "samples_per_bin = 25  #@param\n",
    "# The highest bin contains 2**-num_bins of the data.\n",
    "top_k = samples_per_bin * 2**(num_bins + 1)\n",
    "\n",
    "embeddings_ds = project_state.create_embeddings_dataset(shuffle_files=True)\n",
    "results, all_logits = search.classifer_search_embeddings_parallel(\n",
    "    embeddings_classifier=model,\n",
    "    target_index=class_list.classes.index(target_class),\n",
    "    random_sample=True,\n",
    "    top_k=top_k,\n",
    "    hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
    "    embeddings_dataset=embeddings_ds,\n",
    ")\n",
    "\n",
    "# Pick samples_per_bin examples from each quantile.\n",
    "def get_quantile_bounds(n_bins):\n",
    "  lowers = [1.0 - 1.0 / 2**(k + 1) for k in range(n_bins - 1)]\n",
    "  return np.array([0.0] + lowers + [1.0])\n",
    "\n",
    "bounds = get_quantile_bounds(num_bins)\n",
    "q_bounds = np.quantile(all_logits, bounds)\n",
    "binned = [[] for _ in range(num_bins)]\n",
    "for r in results.search_results:\n",
    "  bin = np.argmax(r.score < q_bounds) - 1\n",
    "  binned[bin].append(r)\n",
    "binned = [np.random.choice(b, samples_per_bin) for b in binned]\n",
    "\n",
    "ys, _, _, = plt.hist(all_logits, bins=100, density=True)\n",
    "for q in q_bounds:\n",
    "  plt.plot([q, q], [0.0, np.max(ys)], 'k:', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "It0gpV_avg7h"
   },
   "outputs": [],
   "source": [
    "#@title Display Results. { vertical-output: true }\n",
    "\n",
    "combined = []\n",
    "for b in binned:\n",
    "  combined.extend(b)\n",
    "np.random.shuffle(combined)\n",
    "\n",
    "samples_per_page = 40 #@param\n",
    "page_state = display.PageState(np.ceil(len(combined) / samples_per_page))\n",
    "\n",
    "display.display_paged_results(\n",
    "    search.TopKSearchResults(combined, len(combined)),\n",
    "    page_state, samples_per_page,\n",
    "    project_state=project_state,\n",
    "    embedding_sample_rate=project_state.embedding_model.sample_rate,\n",
    "    exclusive_labels=True,\n",
    "    checkbox_labels=[target_class, f'not {target_class}', 'unsure'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "A30usazfu8h8"
   },
   "outputs": [],
   "source": [
    "#@title Collate results and write validation log. { vertical-output: true }\n",
    "\n",
    "validation_log_filepath = epath.Path(working_dir) / 'validation.csv'\n",
    "\n",
    "filenames = []\n",
    "timestamp_offsets = []\n",
    "scores = []\n",
    "is_pos = []\n",
    "weights = []\n",
    "bins = []\n",
    "\n",
    "# Compute the sampling weights for each bin.\n",
    "bin_wts = [1.0 / 2**(k + 1) for k in range(num_bins - 1)]\n",
    "bin_wts.append(bin_wts[-1])\n",
    "\n",
    "for r in combined:\n",
    "  if not r.label_widgets: continue\n",
    "  value = r.label_widgets[0].value\n",
    "  if value is None:\n",
    "    continue\n",
    "  filenames.append(r.filename)\n",
    "  scores.append(r.score)\n",
    "  timestamp_offsets.append(r.timestamp_offset)\n",
    "\n",
    "  # Get the bin number and sampling weight for the search result.\n",
    "  bin = np.argmax(r.score < q_bounds) - 1\n",
    "  bins.append(bin)\n",
    "  weights.append(bin_wts[bin])\n",
    "\n",
    "  if value == target_class:\n",
    "    is_pos.append(1)\n",
    "  elif value == f'not {target_class}':\n",
    "    is_pos.append(-1)\n",
    "  elif value == 'unsure':\n",
    "    is_pos.append(0)\n",
    "\n",
    "label = [target_class for _ in range(len(filenames))]\n",
    "log = pd.DataFrame({\n",
    "    'filenames': filenames,\n",
    "    'timestamp_offsets': timestamp_offsets,\n",
    "    'scores': scores,\n",
    "    'is_pos': is_pos,\n",
    "    'weights': weights,\n",
    "    'bins': bins,\n",
    "})\n",
    "log.to_csv(validation_log_filepath, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uZHVzPttwGZ2"
   },
   "outputs": [],
   "source": [
    "#@title Estimate Call Density. { vertical-output: true }\n",
    "\n",
    "import scipy\n",
    "\n",
    "# Collect validated labels by bin.\n",
    "bin_pos = [0 for i in range(num_bins)]\n",
    "bin_neg = [0 for i in range(num_bins)]\n",
    "for score, pos in zip(scores, is_pos):\n",
    "  bin = np.argmax(score < q_bounds) - 1\n",
    "  if pos == 1:\n",
    "    bin_pos[bin] += 1\n",
    "  elif pos == -1:\n",
    "    bin_neg[bin] += 1\n",
    "\n",
    "# Create beta distributions.\n",
    "prior = 0.1\n",
    "betas = [scipy.stats.beta(p + prior, n + prior)\n",
    "         for p, n in zip(bin_pos, bin_neg)]\n",
    "# MLE positive rate in each bin.\n",
    "mle_b = np.array([bin_pos[b] / (bin_pos[b] + bin_neg[b] + 1e-6)\n",
    "                  for b in range(num_bins)])\n",
    "# Probability of each bin, P(b).\n",
    "p_b = np.array([2**-k for k in range(1, num_bins)] + [2**(-num_bins + 1)])\n",
    "\n",
    "# MLE total call density.\n",
    "q_mle = np.dot(mle_b, p_b)\n",
    "\n",
    "num_beta_samples = 10_000\n",
    "q_betas = []\n",
    "for _ in range(num_beta_samples):\n",
    "  qs_pos = np.array([b.rvs(size=1)[0] for b in betas])  # P(+|b)\n",
    "  q_beta = np.dot(qs_pos, p_b)\n",
    "  q_betas.append(q_beta)\n",
    "\n",
    "# Plot call density estimate.\n",
    "plt.figure(figsize=(10, 5))\n",
    "xs, ys, _ = plt.hist(q_betas, density=True, bins=25, alpha=0.25)\n",
    "plt.plot([q_mle, q_mle], [0.0, np.max(xs)], 'k:', alpha=0.75,\n",
    "         label='q_mle')\n",
    "\n",
    "low, high = np.quantile(q_betas, [0.05, 0.95])\n",
    "plt.plot([low, low], [0.0, np.max(xs)], 'g', alpha=0.75, label='low conf')\n",
    "plt.plot([high, high], [0.0, np.max(xs)], 'g', alpha=0.75, label='high conf')\n",
    "\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.xlabel('Call Rate (q)')\n",
    "plt.ylabel('P(q)')\n",
    "plt.title(f'Call Density Estimation ({target_class})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'MLE Call Density: {q_mle:.4f}')\n",
    "print(f'(Low/MLE/High) Call Density Estimate: ({low:5.4f} / {q_mle:5.4f} / {high:5.4f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "6PPrCBc-15k_"
   },
   "outputs": [],
   "source": [
    "#@title Naive Estimation of ROC-AUC for target class. { vertical-output: true }\n",
    "# Computes ROC-AUC from the validation logs, with bin weighting.\n",
    "# ROC-AUC is the overall probability that a random positive example\n",
    "# has a higher classifier score than a random negative example.\n",
    "\n",
    "# Probability of bins\n",
    "p_b = [1.0 / 2**(k + 1) for k in range(num_bins - 1)]\n",
    "p_b.append(p_b[-1])\n",
    "p_b = np.array(p_b)\n",
    "\n",
    "bin_pos, bin_neg = np.array(bin_pos), np.array(bin_neg)\n",
    "# Compute P(z(+) > z(-))\n",
    "# P(b_i|+) * P(b_k|-)\n",
    "n_pos = np.sum(bin_pos)\n",
    "n_neg = np.sum(bin_neg)\n",
    "p_pos_b = np.array(bin_pos) / (bin_pos + bin_neg)\n",
    "p_neg_b = np.array(bin_neg) / (bin_pos + bin_neg)\n",
    "p_pos = np.sum(p_pos_b * p_b)\n",
    "p_neg = np.sum(p_neg_b * p_b)\n",
    "\n",
    "p_b_pos = p_pos_b * p_b / p_pos\n",
    "p_b_neg = p_neg_b * p_b / p_neg\n",
    "roc_auc = 0\n",
    "# For off-diagonal bin pairs:\n",
    "# Take the probability of drawing a pos from bin j and neg from bin i.\n",
    "# If j > i, all pos examples are scored higher, so contributes directly to the\n",
    "# total ROC-AUC.\n",
    "for i in range(num_bins):\n",
    "  for j in range(i + 1, num_bins):\n",
    "    roc_auc += p_b_pos[j] * p_b_neg[i]\n",
    "\n",
    "# For diagonal bin-pairs:\n",
    "# Look at actual in-bin observations for diagonal contribution.\n",
    "bins = np.array(bins)\n",
    "is_pos = np.array(is_pos)\n",
    "\n",
    "for b in range(num_bins):\n",
    "  bin_pos_idxes = np.argwhere((bins == b) * (is_pos == 1))[:, 0]\n",
    "  bin_neg_idxes = np.argwhere((bins == b) * (is_pos == -1))[:, 0]\n",
    "  bin_pos_scores = np.array(scores)[bin_pos_idxes]\n",
    "  bin_neg_scores = np.array(scores)[bin_neg_idxes]\n",
    "  if bin_pos_scores.size == 0:\n",
    "    continue\n",
    "  if bin_neg_scores.size == 0:\n",
    "    continue\n",
    "  # Count total number of pairs where the pos examples have a higher score than\n",
    "  # a negative example.\n",
    "  hits = ((bin_pos_scores[:, np.newaxis]\n",
    "           - bin_neg_scores[np.newaxis, :]) > 0).sum()\n",
    "  bin_roc_auc = hits / (bin_pos_scores.size * bin_neg_scores.size)\n",
    "  # Contribution is the probability of pulling both pos and neg examples\n",
    "  # from this bin, multiplied by the bin's ROC-AUC.\n",
    "  roc_auc += bin_roc_auc * p_b_pos[b] * p_b_neg[b]\n",
    "\n",
    "print(f'ROC-AUC : {roc_auc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
